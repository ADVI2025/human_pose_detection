{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f677cd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: filterpy in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (1.4.5)\n",
      "Requirement already satisfied: scipy in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (1.15.3)\n",
      "Requirement already satisfied: matplotlib in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (3.10.5)\n",
      "Requirement already satisfied: opencv-contrib-python in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: numpy<2 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: absl-py in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sentencepiece in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: jaxlib in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: jax in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pillow>=8 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from jax->mediapipe) (0.5.3)\n",
      "Requirement already satisfied: opt_einsum in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: pycparser in d:\\fitz\\fitzpatrick17k-main\\fzenv\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python filterpy scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9142c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames extracted: 240\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess_video(video_path, target_fps=30, target_size=(1280, 720), background_subtraction=False):\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video: {video_path}\")\n",
    "        return []\n",
    "\n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_interval = int(orig_fps / target_fps) if orig_fps > target_fps else 1\n",
    "\n",
    "    if background_subtraction:\n",
    "        back_sub = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16, detectShadows=True)\n",
    "        # or\n",
    "        # back_sub = cv2.createBackgroundSubtractorKNN()\n",
    "\n",
    "    frames = []\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx % frame_interval == 0:\n",
    "            # Resize frame\n",
    "            frame_resized = cv2.resize(frame, target_size)\n",
    "\n",
    "            # Convert to HSV for color normalization if needed or keep in BGR\n",
    "            # Histogram equalization per channel\n",
    "            frame_yuv = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2YUV)\n",
    "            frame_yuv[:, :, 0] = cv2.equalizeHist(frame_yuv[:, :, 0])  \n",
    "            frame_eq = cv2.cvtColor(frame_yuv, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "            # Background subtraction\n",
    "            if background_subtraction:\n",
    "                fg_mask = back_sub.apply(frame_eq)\n",
    "                frame_eq = cv2.bitwise_and(frame_eq, frame_eq, mask=fg_mask)\n",
    "\n",
    "            frames.append(frame_eq)\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Example \n",
    "# video_path = r\"D:\\fitz\\fitzpatrick17k-main\\pose\\UCF50\\Drumming\\v_Drumming_g01_c01.avi\"\n",
    "video_path = r\"D:\\fitz\\fitzpatrick17k-main\\pose\\UCF50\\WalkingWithDog\\v_WalkingWithDog_g01_c01.avi\"\n",
    "# video_path = r\"D:\\fitz\\fitzpatrick17k-main\\pose\\UCF50\\PullUps\\v_Pullup_g01_c01.avi\"\n",
    "\n",
    "frames = preprocess_video(video_path, background_subtraction=True)\n",
    "print(f\"Total frames extracted: {len(frames)}\")\n",
    "\n",
    "# Optionally display first frame for verification\n",
    "if frames:\n",
    "    cv2.imshow(\"Preprocessed Frame\", frames[0])\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb68400f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f61437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_kalman_filter():\n",
    "    kf = KalmanFilter(dim_x=4, dim_z=2)\n",
    "    kf.F = np.array([[1, 0, 1, 0],\n",
    "                     [0, 1, 0, 1],\n",
    "                     [0, 0, 1, 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "    kf.H = np.array([[1, 0, 0, 0],\n",
    "                     [0, 1, 0, 0]])\n",
    "    kf.P *= 1000\n",
    "    kf.R = np.eye(2) * 5\n",
    "    kf.Q = np.eye(4)\n",
    "    return kf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca33f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a979fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_state = 'STATIC'\n",
    "static_count = 0\n",
    "\n",
    "def classify_joint_movement(delta_y, angle_velocity, epsilon_y=5, epsilon_theta=2, static_threshold=3):\n",
    "    global last_state, static_count\n",
    "    \n",
    "    if delta_y < -epsilon_y and angle_velocity > epsilon_theta:\n",
    "        last_state = 'UP'\n",
    "        static_count = 0\n",
    "        return 'UP'\n",
    "    elif delta_y > epsilon_y and angle_velocity < -epsilon_theta:\n",
    "        last_state = 'DOWN'\n",
    "        static_count = 0\n",
    "        return 'DOWN'\n",
    "    elif abs(delta_y) < epsilon_y and abs(angle_velocity) < epsilon_theta:\n",
    "        static_count += 1\n",
    "        if static_count >= static_threshold:\n",
    "            return 'STATIC'\n",
    "        else:\n",
    "            return last_state\n",
    "    else:\n",
    "        static_count = 0\n",
    "        return 'TRANSITION'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f1d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for different body parts: (name, joint triplet IDs)\n",
    "JOINT_TRIPLETS = {\n",
    "    \"right_elbow\": (12, 14, 16),  # Right shoulder, right elbow, right wrist\n",
    "    \"left_elbow\": (11, 13, 15),   # Left shoulder, left elbow, left wrist\n",
    "    \"right_knee\": (24, 26, 28),   # Right hip, right knee, right ankle\n",
    "    \"left_knee\": (23, 25, 27)     # Left hip, left knee, left ankle\n",
    "}\n",
    "\n",
    "def process_joint_movements(image, joint_history, angle_history):\n",
    "    y_offset = 50  # Starting position for text\n",
    "    for name, (A_id, B_id, C_id) in JOINT_TRIPLETS.items():\n",
    "        # Get latest coordinates\n",
    "        if len(joint_history.get(f'joint_{B_id}', [])) < 2:\n",
    "            continue \n",
    "\n",
    "        A = joint_history[f'joint_{A_id}'][-1]\n",
    "        B = joint_history[f'joint_{B_id}'][-1]\n",
    "        C = joint_history[f'joint_{C_id}'][-1]\n",
    "\n",
    "        # Calculate angle\n",
    "        angle = calculate_angle(A, B, C)\n",
    "        angle_history.setdefault(name, []).append(angle)\n",
    "\n",
    "        # Calculate movement parameters\n",
    "        delta_y = B[1] - joint_history[f'joint_{B_id}'][-2][1]\n",
    "        angle_velocity = 0\n",
    "        if len(angle_history[name]) > 1:\n",
    "            angle_velocity = angle - angle_history[name][-2]\n",
    "\n",
    "        # Classify movement\n",
    "        label = classify_joint_movement(delta_y, angle_velocity)\n",
    "\n",
    "        # Display on image\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            f\"{name.replace('_', ' ').title()}: {label} Angle: {angle:.2f}\",\n",
    "            (30, y_offset),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.9,\n",
    "            (255, 0, 0),\n",
    "            2\n",
    "        )\n",
    "        y_offset += 40  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aeac3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define joint index mappings (Mediapipe Pose IDs)\n",
    "ANGLE_JOINTS = {\n",
    "    \"right_elbow\": (12, 14, 16),  # Shoulder, Elbow, Wrist\n",
    "    \"left_elbow\": (11, 13, 15),\n",
    "    \"right_knee\": (24, 26, 28),   # Hip, Knee, Ankle\n",
    "    \"left_knee\": (23, 25, 27),\n",
    "    # \"spine\": (11, 23, 24)         # Left Shoulder, Left Hip, Right Hip\n",
    "    \"left_hip_knee\": (11, 23, 25),     # Left Shoulder, Left Hip, Left Knee\n",
    "    \"right_hip_knee\": (12, 24, 26) \n",
    "}\n",
    "\n",
    "# Colors for each joint label\n",
    "JOINT_COLORS = {\n",
    "    \"right_elbow\": (255, 0, 0),\n",
    "    \"left_elbow\": (0, 255, 0),\n",
    "    \"right_knee\": (0, 0, 255),\n",
    "    \"left_knee\": (255, 255, 0),\n",
    "    \"left_hip_knee\": (255, 0, 255),\n",
    "    \"right_hip_knee\": (0, 255, 255)\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a634ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def draw_trajectories(image, joint_history, max_len=20):\n",
    "    \"\"\"Draw trailing lines showing movement history for each joint.\"\"\"\n",
    "    for key, points in joint_history.items():\n",
    "        if len(points) < 2:\n",
    "            continue\n",
    "        color = (0, 255, 255)  # Example trail color: cyan\n",
    "        for i in range(max(0, len(points)-max_len), len(points)-1):\n",
    "            cv2.line(image, points[i], points[i+1], color, 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_csv_log(filename, angle_history, joint_history, status_history):\n",
    "    \"\"\"Save angle, joint positions, and status to CSV file.\"\"\"\n",
    "    with open(filename, mode='w', newline='') as csv_file:\n",
    "        fieldnames = ['frame', 'joint', 'angle', 'x', 'y', 'status']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        frames = len(next(iter(angle_history.values()), []))\n",
    "        for frame_idx in range(frames):\n",
    "            for joint_name in ANGLE_JOINTS.keys():\n",
    "                angle = angle_history.get(joint_name, [None]*frames)[frame_idx]\n",
    "                # Use last known joint b (middle joint) position as example\n",
    "                b_idx = ANGLE_JOINTS[joint_name][1]\n",
    "                pos = joint_history.get(f'joint_{b_idx}', [(None, None)]*frames)[frame_idx]\n",
    "                status = status_history.get(joint_name, [\"\"]*frames)[frame_idx]\n",
    "                writer.writerow({\n",
    "                    'frame': frame_idx,\n",
    "                    'joint': joint_name,\n",
    "                    'angle': angle,\n",
    "                    'x': pos[0],\n",
    "                    'y': pos[1],\n",
    "                    'status': status\n",
    "                })\n",
    "\n",
    "\n",
    "def write_video(output_path, frame_width, frame_height, fps=30):\n",
    "    \"\"\"Return a VideoWriter object for saving output.\"\"\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    return cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93baac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_kalman_filters = {}\n",
    "joint_history = {}\n",
    "angle_history = {}\n",
    "frame_height, frame_width = frames[0].shape[:2]\n",
    "output_video_path = \"output_annotated.avi\"\n",
    "video_writer = write_video(output_video_path, frame_width, frame_height, fps=30)\n",
    "status_history = {joint: [] for joint in ANGLE_JOINTS.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70affa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_14448\\2424792725.py:43: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pred_x, pred_y = int(kf.x[0]), int(kf.x[1])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "video_path = r\"D:\\fitz\\fitzpatrick17k-main\\pose\\UCF50\\GolfSwing\\v_GolfSwing_g01_c01.avi\"\n",
    "\n",
    "# video_path = r\"D:\\fitz\\fitzpatrick17k-main\\pose\\UCF50\\PullUps\\v_Pullup_g01_c01.avi\"\n",
    "frames = preprocess_video(video_path, background_subtraction=False)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    for frame in frames:\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "        )\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "                key = f'joint_{idx}'\n",
    "                x, y = int(lm.x * image.shape[1]), int(lm.y * image.shape[0])\n",
    "\n",
    "                if key not in pose_kalman_filters:\n",
    "                    pose_kalman_filters[key] = create_kalman_filter()\n",
    "                    # pose_kalman_filters[key].statePre = np.array([[x], [y], [0], [0]], np.float32)\n",
    "                    pose_kalman_filters[key].x[:2] = np.array([[x], [y]])\n",
    "\n",
    "                kf = pose_kalman_filters[key]\n",
    "                kf.predict()\n",
    "                # kf.correct(np.array([[np.float32(x)], [np.float32(y)]]))\n",
    "                kf.update(np.array([x, y]))\n",
    "                pred_x, pred_y = int(kf.x[0]), int(kf.x[1])\n",
    "\n",
    "                if key not in joint_history:\n",
    "                    joint_history[key] = []\n",
    "                joint_history[key].append((pred_x, pred_y))\n",
    "\n",
    "            # Process angles & movements (as you have it)\n",
    "            for name, (a_idx, b_idx, c_idx) in ANGLE_JOINTS.items():\n",
    "                if all(len(joint_history.get(f'joint_{i}', [])) >= 2 for i in (a_idx, b_idx, c_idx)):\n",
    "                    a = joint_history[f'joint_{a_idx}'][-1]\n",
    "                    b = joint_history[f'joint_{b_idx}'][-1]\n",
    "                    c = joint_history[f'joint_{c_idx}'][-1]\n",
    "\n",
    "                    angle = calculate_angle(a, b, c)\n",
    "                    angle_history.setdefault(name, []).append(angle)\n",
    "\n",
    "                    delta_y = b[1] - joint_history[f'joint_{b_idx}'][-2][1]\n",
    "                    angle_velocity = angle - angle_history[name][-2] if len(angle_history[name]) > 1 else 0\n",
    "\n",
    "                    label = classify_joint_movement(delta_y, angle_velocity)\n",
    "\n",
    "                    cv2.putText(\n",
    "                        image,\n",
    "                        f\"{name.replace('_', ' ').title()}: {label} {int(angle)}°\",\n",
    "                        (b[0] + 10, b[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                        JOINT_COLORS[name], 2\n",
    "                    )\n",
    "                    status_history[name].append(label)\n",
    "        posture = detect_posture(angle_history)\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            f\"Posture: {posture}\",\n",
    "            (30, 30),  # top-left corner\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1.0,\n",
    "            (0, 255, 255), 2\n",
    "        )\n",
    "        # draw_trajectories(image, joint_history)\n",
    "\n",
    "\n",
    "        cv2.imshow('Pose Tracking', image)\n",
    "        video_writer.write(image)\n",
    "\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32be237c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_32584\\2059409268.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pred_x, pred_y = int(kf.x[0]), int(kf.x[1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "        )\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "                key = f'joint_{idx}'\n",
    "                x, y = int(lm.x * frame.shape[1]), int(lm.y * frame.shape[0])\n",
    "\n",
    "                if key not in pose_kalman_filters:\n",
    "                    pose_kalman_filters[key] = create_kalman_filter()\n",
    "                    pose_kalman_filters[key].x[:2] = np.array([[x], [y]])\n",
    "\n",
    "                kf = pose_kalman_filters[key]\n",
    "                kf.predict()\n",
    "                kf.update(np.array([x, y]))\n",
    "                pred_x, pred_y = int(kf.x[0]), int(kf.x[1])\n",
    "\n",
    "                if key not in joint_history:\n",
    "                    joint_history[key] = []\n",
    "                joint_history[key].append((pred_x, pred_y))\n",
    "                # cv2.circle(image, (pred_x, pred_y), 4, (0, 255, 0), -1)\n",
    "\n",
    "            # Process angles & movements\n",
    "            for name, (a_idx, b_idx, c_idx) in ANGLE_JOINTS.items():\n",
    "                if all(len(joint_history.get(f'joint_{i}', [])) >= 2 for i in (a_idx, b_idx, c_idx)):\n",
    "                    a = joint_history[f'joint_{a_idx}'][-1]\n",
    "                    b = joint_history[f'joint_{b_idx}'][-1]\n",
    "                    c = joint_history[f'joint_{c_idx}'][-1]\n",
    "\n",
    "                    angle = calculate_angle(a, b, c)\n",
    "                    angle_history.setdefault(name, []).append(angle)\n",
    "\n",
    "                    delta_y = b[1] - joint_history[f'joint_{b_idx}'][-2][1]\n",
    "                    angle_velocity = angle - angle_history[name][-2] if len(angle_history[name]) > 1 else 0\n",
    "\n",
    "                    label = classify_joint_movement(delta_y, angle_velocity)\n",
    "\n",
    "                    cv2.putText(\n",
    "                        image,\n",
    "                        f\"{name.replace('_', ' ').title()}: {label} {int(angle)}°\",\n",
    "                        (b[0] + 10, b[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                        JOINT_COLORS[name], 2\n",
    "                    )\n",
    "\n",
    "        cv2.imshow('Pose Tracking', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08846b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b1a8fc7",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====== Your existing imports/functions should be above ======\n",
    "# Assuming you already have: preprocess_video, create_kalman_filter, calculate_angle, classify_joint_movement, ANGLE_JOINTS\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Output arrays\n",
    "all_sequences = []\n",
    "all_labels = []\n",
    "\n",
    "# Root dataset path\n",
    "DATASET_PATH = r\"train\"  # change to your dataset folder\n",
    "actions = os.listdir(DATASET_PATH)  # [\"Fall Down\", \"Lying Down\", ...]\n",
    "\n",
    "pose_kalman_filters = {}\n",
    "joint_history = {}\n",
    "angle_history = {}\n",
    "status_history = {}\n",
    "\n",
    "for action in actions:\n",
    "    action_path = os.path.join(DATASET_PATH, action)\n",
    "    videos = [f for f in os.listdir(action_path) if f.lower().endswith(('.mp4', '.avi'))]\n",
    "\n",
    "    for video_file in tqdm(videos, desc=f\"Processing {action}\"):\n",
    "        video_path = os.path.join(action_path, video_file)\n",
    "\n",
    "        # Preprocess frames\n",
    "        frames = preprocess_video(video_path, background_subtraction=False)\n",
    "\n",
    "        sequence_features = []  # one sequence per video\n",
    "\n",
    "        with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "            for frame in frames:\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = pose.process(image)\n",
    "\n",
    "                if results.pose_landmarks:\n",
    "                    frame_features = []\n",
    "                    for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "                        x, y = int(lm.x * frame.shape[1]), int(lm.y * frame.shape[0])\n",
    "\n",
    "                        key = f'joint_{idx}'\n",
    "                        if key not in pose_kalman_filters:\n",
    "                            pose_kalman_filters[key] = create_kalman_filter()\n",
    "                            pose_kalman_filters[key].x[:2] = np.array([[x], [y]])\n",
    "                        kf = pose_kalman_filters[key]\n",
    "                        kf.predict()\n",
    "                        kf.update(np.array([x, y]))\n",
    "                        pred_x, pred_y = int(kf.x[0]), int(kf.x[1])\n",
    "\n",
    "                        # Append positions\n",
    "                        frame_features.extend([pred_x, pred_y])\n",
    "\n",
    "                    # Angles + status\n",
    "                    for name, (a_idx, b_idx, c_idx) in ANGLE_JOINTS.items():\n",
    "                        if all(joint_history.get(f'joint_{i}', []) for i in (a_idx, b_idx, c_idx)):\n",
    "                            a = (pose_kalman_filters[f'joint_{a_idx}'].x[0], pose_kalman_filters[f'joint_{a_idx}'].x[1])\n",
    "                            b = (pose_kalman_filters[f'joint_{b_idx}'].x[0], pose_kalman_filters[f'joint_{b_idx}'].x[1])\n",
    "                            c = (pose_kalman_filters[f'joint_{c_idx}'].x[0], pose_kalman_filters[f'joint_{c_idx}'].x[1])\n",
    "\n",
    "                            angle = calculate_angle(a, b, c)\n",
    "                            delta_y = b[1] - joint_history.get(f'joint_{b_idx}', [(0,0)])[-1][1]\n",
    "                            angle_velocity = 0\n",
    "                            if name in angle_history and len(angle_history[name]) > 0:\n",
    "                                angle_velocity = angle - angle_history[name][-1]\n",
    "\n",
    "                            label = classify_joint_movement(delta_y, angle_velocity)\n",
    "\n",
    "                            frame_features.extend([angle, status_history.get(name, [\"STATIC\"])[-1] if name in status_history else \"STATIC\"])\n",
    "                        else:\n",
    "                            frame_features.extend([0, \"STATIC\"])\n",
    "\n",
    "                    # Replace status strings with numbers\n",
    "                    status_map = {\"UP\": 1, \"DOWN\": -1, \"STATIC\": 0}\n",
    "                    frame_features = [status_map.get(f, f) if isinstance(f, str) else f for f in frame_features]\n",
    "\n",
    "                    sequence_features.append(frame_features)\n",
    "\n",
    "        if sequence_features:\n",
    "            all_sequences.append(sequence_features)\n",
    "            all_labels.append(action)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(all_sequences, dtype=object) \n",
    "y = np.array(all_labels)\n",
    "\n",
    "np.save(\"X_sequences.npy\", X)\n",
    "np.save(\"y_labels.npy\", y)\n",
    "\n",
    "print(f\"Saved {len(X)} sequences with labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f2c8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PoseSequenceTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, d_model=128, nhead=8, num_layers=4, dim_feedforward=256, dropout=0.1):\n",
    "        super(PoseSequenceTransformer, self).__init__()\n",
    "        \n",
    "        # Linear layer to map input features to d_model dimension\n",
    "        self.input_fc = nn.Linear(num_features, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc_out = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_fc(src)  # [B, T, F] -> [B, T, d_model]\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.mean(dim=1)  # Global average pooling over time\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1721c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class PoseSequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = torch.tensor(self.X[idx], dtype=torch.float32)  # [T, F]\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return seq, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_seqs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad8abce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "cuda\n",
      "\n",
      "===== Fold 1 =====\n",
      "Epoch [1/50] - Loss: 1.8396 - Train Acc: 0.2728 - Val Acc: 0.3170\n",
      "Epoch [2/50] - Loss: 1.5382 - Train Acc: 0.3676 - Val Acc: 0.3340\n",
      "Epoch [3/50] - Loss: 1.4464 - Train Acc: 0.4030 - Val Acc: 0.3472\n",
      "Epoch [4/50] - Loss: 1.4303 - Train Acc: 0.4143 - Val Acc: 0.3491\n",
      "Epoch [5/50] - Loss: 1.3865 - Train Acc: 0.4502 - Val Acc: 0.3906\n",
      "Epoch [6/50] - Loss: 1.3318 - Train Acc: 0.4620 - Val Acc: 0.4038\n",
      "Epoch [7/50] - Loss: 1.3080 - Train Acc: 0.4587 - Val Acc: 0.4245\n",
      "Epoch [8/50] - Loss: 1.3206 - Train Acc: 0.4714 - Val Acc: 0.4000\n",
      "Epoch [9/50] - Loss: 1.2581 - Train Acc: 0.4922 - Val Acc: 0.4962\n",
      "Epoch [10/50] - Loss: 1.2174 - Train Acc: 0.5300 - Val Acc: 0.4491\n",
      "Epoch [11/50] - Loss: 1.1565 - Train Acc: 0.5517 - Val Acc: 0.5113\n",
      "Epoch [12/50] - Loss: 1.1081 - Train Acc: 0.5639 - Val Acc: 0.5019\n",
      "Epoch [13/50] - Loss: 1.0896 - Train Acc: 0.5805 - Val Acc: 0.5226\n",
      "Epoch [14/50] - Loss: 1.0554 - Train Acc: 0.6083 - Val Acc: 0.5283\n",
      "Epoch [15/50] - Loss: 1.0609 - Train Acc: 0.6012 - Val Acc: 0.5189\n",
      "Epoch [16/50] - Loss: 1.0603 - Train Acc: 0.5908 - Val Acc: 0.5208\n",
      "Epoch [17/50] - Loss: 1.0308 - Train Acc: 0.5946 - Val Acc: 0.5528\n",
      "Epoch [18/50] - Loss: 1.0268 - Train Acc: 0.5932 - Val Acc: 0.5642\n",
      "Epoch [19/50] - Loss: 1.0048 - Train Acc: 0.6192 - Val Acc: 0.5358\n",
      "Epoch [20/50] - Loss: 1.0099 - Train Acc: 0.6201 - Val Acc: 0.5736\n",
      "Epoch [21/50] - Loss: 0.9464 - Train Acc: 0.6220 - Val Acc: 0.5528\n",
      "Epoch [22/50] - Loss: 0.9195 - Train Acc: 0.6352 - Val Acc: 0.5717\n",
      "Epoch [23/50] - Loss: 0.9098 - Train Acc: 0.6446 - Val Acc: 0.5566\n",
      "Epoch [24/50] - Loss: 0.9229 - Train Acc: 0.6475 - Val Acc: 0.5981\n",
      "Epoch [25/50] - Loss: 0.9216 - Train Acc: 0.6361 - Val Acc: 0.5925\n",
      "Epoch [26/50] - Loss: 0.8985 - Train Acc: 0.6527 - Val Acc: 0.5943\n",
      "Epoch [27/50] - Loss: 0.8845 - Train Acc: 0.6531 - Val Acc: 0.5887\n",
      "Epoch [28/50] - Loss: 0.8775 - Train Acc: 0.6597 - Val Acc: 0.6000\n",
      "Epoch [29/50] - Loss: 0.8998 - Train Acc: 0.6376 - Val Acc: 0.6075\n",
      "Epoch [30/50] - Loss: 0.8664 - Train Acc: 0.6654 - Val Acc: 0.6170\n",
      "Epoch [31/50] - Loss: 0.8156 - Train Acc: 0.6857 - Val Acc: 0.6245\n",
      "Epoch [32/50] - Loss: 0.8132 - Train Acc: 0.6763 - Val Acc: 0.6000\n",
      "Epoch [33/50] - Loss: 0.8191 - Train Acc: 0.6819 - Val Acc: 0.5981\n",
      "Epoch [34/50] - Loss: 0.8109 - Train Acc: 0.6819 - Val Acc: 0.6132\n",
      "Epoch [35/50] - Loss: 0.8205 - Train Acc: 0.6800 - Val Acc: 0.6170\n",
      "Epoch [36/50] - Loss: 0.7873 - Train Acc: 0.6876 - Val Acc: 0.6132\n",
      "Epoch [37/50] - Loss: 0.7832 - Train Acc: 0.6937 - Val Acc: 0.6226\n",
      "Epoch [38/50] - Loss: 0.7999 - Train Acc: 0.6833 - Val Acc: 0.6226\n",
      "Epoch [39/50] - Loss: 0.7869 - Train Acc: 0.6881 - Val Acc: 0.6358\n",
      "Epoch [40/50] - Loss: 0.7742 - Train Acc: 0.6980 - Val Acc: 0.6434\n",
      "Epoch [41/50] - Loss: 0.7637 - Train Acc: 0.7112 - Val Acc: 0.6358\n",
      "Epoch [42/50] - Loss: 0.7614 - Train Acc: 0.7046 - Val Acc: 0.6396\n",
      "Epoch [43/50] - Loss: 0.7409 - Train Acc: 0.7060 - Val Acc: 0.6358\n",
      "Epoch [44/50] - Loss: 0.7263 - Train Acc: 0.7135 - Val Acc: 0.6340\n",
      "Epoch [45/50] - Loss: 0.7301 - Train Acc: 0.7055 - Val Acc: 0.6528\n",
      "Epoch [46/50] - Loss: 0.7472 - Train Acc: 0.7117 - Val Acc: 0.6302\n",
      "Epoch [47/50] - Loss: 0.7413 - Train Acc: 0.7131 - Val Acc: 0.6358\n",
      "Epoch [48/50] - Loss: 0.7329 - Train Acc: 0.7206 - Val Acc: 0.6302\n",
      "Epoch [49/50] - Loss: 0.7458 - Train Acc: 0.7107 - Val Acc: 0.6453\n",
      "Epoch [50/50] - Loss: 0.7273 - Train Acc: 0.7168 - Val Acc: 0.6377\n",
      "\n",
      "===== Fold 2 =====\n",
      "Epoch [1/50] - Loss: 1.6782 - Train Acc: 0.3101 - Val Acc: 0.3547\n",
      "Epoch [2/50] - Loss: 1.4537 - Train Acc: 0.4025 - Val Acc: 0.3887\n",
      "Epoch [3/50] - Loss: 1.3937 - Train Acc: 0.4427 - Val Acc: 0.3679\n",
      "Epoch [4/50] - Loss: 1.3754 - Train Acc: 0.4460 - Val Acc: 0.4604\n",
      "Epoch [5/50] - Loss: 1.3661 - Train Acc: 0.4479 - Val Acc: 0.4245\n",
      "Epoch [6/50] - Loss: 1.3559 - Train Acc: 0.4601 - Val Acc: 0.4396\n",
      "Epoch [7/50] - Loss: 1.3346 - Train Acc: 0.4705 - Val Acc: 0.4736\n",
      "Epoch [8/50] - Loss: 1.2759 - Train Acc: 0.4974 - Val Acc: 0.4358\n",
      "Epoch [9/50] - Loss: 1.2916 - Train Acc: 0.4837 - Val Acc: 0.4208\n",
      "Epoch [10/50] - Loss: 1.1942 - Train Acc: 0.5252 - Val Acc: 0.5358\n",
      "Epoch [11/50] - Loss: 1.1424 - Train Acc: 0.5319 - Val Acc: 0.5509\n",
      "Epoch [12/50] - Loss: 1.1317 - Train Acc: 0.5672 - Val Acc: 0.5566\n",
      "Epoch [13/50] - Loss: 1.1325 - Train Acc: 0.5498 - Val Acc: 0.5830\n",
      "Epoch [14/50] - Loss: 1.0742 - Train Acc: 0.5809 - Val Acc: 0.5868\n",
      "Epoch [15/50] - Loss: 1.0664 - Train Acc: 0.5753 - Val Acc: 0.5623\n",
      "Epoch [16/50] - Loss: 1.0329 - Train Acc: 0.5937 - Val Acc: 0.5755\n",
      "Epoch [17/50] - Loss: 1.0328 - Train Acc: 0.5951 - Val Acc: 0.6170\n",
      "Epoch [18/50] - Loss: 1.0299 - Train Acc: 0.5937 - Val Acc: 0.5698\n",
      "Epoch [19/50] - Loss: 1.0109 - Train Acc: 0.6041 - Val Acc: 0.5472\n",
      "Epoch [20/50] - Loss: 1.0491 - Train Acc: 0.5871 - Val Acc: 0.5925\n",
      "Epoch [21/50] - Loss: 0.9867 - Train Acc: 0.6295 - Val Acc: 0.6189\n",
      "Epoch [22/50] - Loss: 0.9783 - Train Acc: 0.6149 - Val Acc: 0.6057\n",
      "Epoch [23/50] - Loss: 0.9394 - Train Acc: 0.6295 - Val Acc: 0.6000\n",
      "Epoch [24/50] - Loss: 0.9185 - Train Acc: 0.6314 - Val Acc: 0.6151\n",
      "Epoch [25/50] - Loss: 0.9237 - Train Acc: 0.6404 - Val Acc: 0.6038\n",
      "Epoch [26/50] - Loss: 0.9226 - Train Acc: 0.6366 - Val Acc: 0.6226\n",
      "Epoch [27/50] - Loss: 0.9217 - Train Acc: 0.6380 - Val Acc: 0.6245\n",
      "Epoch [28/50] - Loss: 0.9021 - Train Acc: 0.6328 - Val Acc: 0.6377\n",
      "Epoch [29/50] - Loss: 0.8751 - Train Acc: 0.6479 - Val Acc: 0.6377\n",
      "Epoch [30/50] - Loss: 0.8648 - Train Acc: 0.6621 - Val Acc: 0.6302\n",
      "Epoch [31/50] - Loss: 0.8377 - Train Acc: 0.6645 - Val Acc: 0.6528\n",
      "Epoch [32/50] - Loss: 0.8381 - Train Acc: 0.6640 - Val Acc: 0.6642\n",
      "Epoch [33/50] - Loss: 0.8315 - Train Acc: 0.6777 - Val Acc: 0.6453\n",
      "Epoch [34/50] - Loss: 0.8140 - Train Acc: 0.6890 - Val Acc: 0.6585\n",
      "Epoch [35/50] - Loss: 0.8021 - Train Acc: 0.6786 - Val Acc: 0.6528\n",
      "Epoch [36/50] - Loss: 0.7962 - Train Acc: 0.6933 - Val Acc: 0.6660\n",
      "Epoch [37/50] - Loss: 0.8090 - Train Acc: 0.6895 - Val Acc: 0.6736\n",
      "Epoch [38/50] - Loss: 0.8038 - Train Acc: 0.6800 - Val Acc: 0.6792\n",
      "Epoch [39/50] - Loss: 0.7854 - Train Acc: 0.6970 - Val Acc: 0.6755\n",
      "Epoch [40/50] - Loss: 0.7887 - Train Acc: 0.6937 - Val Acc: 0.6585\n",
      "Epoch [41/50] - Loss: 0.7585 - Train Acc: 0.7003 - Val Acc: 0.6491\n",
      "Epoch [42/50] - Loss: 0.7632 - Train Acc: 0.6975 - Val Acc: 0.6585\n",
      "Epoch [43/50] - Loss: 0.7646 - Train Acc: 0.7074 - Val Acc: 0.6698\n",
      "Epoch [44/50] - Loss: 0.7533 - Train Acc: 0.7150 - Val Acc: 0.6679\n",
      "Epoch [45/50] - Loss: 0.7446 - Train Acc: 0.7131 - Val Acc: 0.6623\n",
      "Epoch [46/50] - Loss: 0.7685 - Train Acc: 0.7084 - Val Acc: 0.6792\n",
      "Epoch [47/50] - Loss: 0.7403 - Train Acc: 0.7088 - Val Acc: 0.6698\n",
      "Epoch [48/50] - Loss: 0.7262 - Train Acc: 0.7131 - Val Acc: 0.6774\n",
      "Epoch [49/50] - Loss: 0.7226 - Train Acc: 0.7150 - Val Acc: 0.6566\n",
      "Epoch [50/50] - Loss: 0.7097 - Train Acc: 0.7268 - Val Acc: 0.6849\n",
      "\n",
      "===== Fold 3 =====\n",
      "Epoch [1/50] - Loss: 1.7069 - Train Acc: 0.2935 - Val Acc: 0.2849\n",
      "Epoch [2/50] - Loss: 1.5401 - Train Acc: 0.3568 - Val Acc: 0.2736\n",
      "Epoch [3/50] - Loss: 1.4778 - Train Acc: 0.4040 - Val Acc: 0.3094\n",
      "Epoch [4/50] - Loss: 1.3867 - Train Acc: 0.4323 - Val Acc: 0.3830\n",
      "Epoch [5/50] - Loss: 1.3526 - Train Acc: 0.4601 - Val Acc: 0.4189\n",
      "Epoch [6/50] - Loss: 1.2920 - Train Acc: 0.4889 - Val Acc: 0.3491\n",
      "Epoch [7/50] - Loss: 1.3136 - Train Acc: 0.4738 - Val Acc: 0.4038\n",
      "Epoch [8/50] - Loss: 1.2358 - Train Acc: 0.5125 - Val Acc: 0.4774\n",
      "Epoch [9/50] - Loss: 1.2181 - Train Acc: 0.5205 - Val Acc: 0.4566\n",
      "Epoch [10/50] - Loss: 1.2314 - Train Acc: 0.5177 - Val Acc: 0.4415\n",
      "Epoch [11/50] - Loss: 1.1164 - Train Acc: 0.5616 - Val Acc: 0.4566\n",
      "Epoch [12/50] - Loss: 1.1034 - Train Acc: 0.5597 - Val Acc: 0.4962\n",
      "Epoch [13/50] - Loss: 1.1757 - Train Acc: 0.5418 - Val Acc: 0.4547\n",
      "Epoch [14/50] - Loss: 1.1255 - Train Acc: 0.5569 - Val Acc: 0.4868\n",
      "Epoch [15/50] - Loss: 1.0954 - Train Acc: 0.5833 - Val Acc: 0.4906\n",
      "Epoch [16/50] - Loss: 1.0783 - Train Acc: 0.5819 - Val Acc: 0.4774\n",
      "Epoch [17/50] - Loss: 1.0823 - Train Acc: 0.5776 - Val Acc: 0.4943\n",
      "Epoch [18/50] - Loss: 1.0647 - Train Acc: 0.5880 - Val Acc: 0.4811\n",
      "Epoch [19/50] - Loss: 1.0648 - Train Acc: 0.5842 - Val Acc: 0.5226\n",
      "Epoch [20/50] - Loss: 1.0753 - Train Acc: 0.5720 - Val Acc: 0.5057\n",
      "Epoch [21/50] - Loss: 0.9994 - Train Acc: 0.6026 - Val Acc: 0.5321\n",
      "Epoch [22/50] - Loss: 0.9681 - Train Acc: 0.6130 - Val Acc: 0.5226\n",
      "Epoch [23/50] - Loss: 0.9839 - Train Acc: 0.6069 - Val Acc: 0.5245\n",
      "Epoch [24/50] - Loss: 0.9591 - Train Acc: 0.6244 - Val Acc: 0.5434\n",
      "Epoch [25/50] - Loss: 0.9680 - Train Acc: 0.6130 - Val Acc: 0.5434\n",
      "Epoch [26/50] - Loss: 0.9414 - Train Acc: 0.6281 - Val Acc: 0.5170\n",
      "Epoch [27/50] - Loss: 0.9391 - Train Acc: 0.6244 - Val Acc: 0.5377\n",
      "Epoch [28/50] - Loss: 0.9160 - Train Acc: 0.6413 - Val Acc: 0.5264\n",
      "Epoch [29/50] - Loss: 0.9486 - Train Acc: 0.6380 - Val Acc: 0.5226\n",
      "Epoch [30/50] - Loss: 0.9310 - Train Acc: 0.6262 - Val Acc: 0.5377\n",
      "Epoch [31/50] - Loss: 0.8661 - Train Acc: 0.6579 - Val Acc: 0.5491\n",
      "Epoch [32/50] - Loss: 0.8344 - Train Acc: 0.6701 - Val Acc: 0.5189\n",
      "Epoch [33/50] - Loss: 0.8489 - Train Acc: 0.6697 - Val Acc: 0.5698\n",
      "Epoch [34/50] - Loss: 0.8396 - Train Acc: 0.6659 - Val Acc: 0.5585\n",
      "Epoch [35/50] - Loss: 0.8400 - Train Acc: 0.6715 - Val Acc: 0.5642\n",
      "Epoch [36/50] - Loss: 0.8304 - Train Acc: 0.6739 - Val Acc: 0.5623\n",
      "Epoch [37/50] - Loss: 0.8335 - Train Acc: 0.6701 - Val Acc: 0.5679\n",
      "Epoch [38/50] - Loss: 0.8360 - Train Acc: 0.6682 - Val Acc: 0.5679\n",
      "Epoch [39/50] - Loss: 0.8125 - Train Acc: 0.6819 - Val Acc: 0.5623\n",
      "Epoch [40/50] - Loss: 0.8104 - Train Acc: 0.6767 - Val Acc: 0.5811\n",
      "Epoch [41/50] - Loss: 0.7845 - Train Acc: 0.6989 - Val Acc: 0.5943\n",
      "Epoch [42/50] - Loss: 0.7825 - Train Acc: 0.6933 - Val Acc: 0.5868\n",
      "Epoch [43/50] - Loss: 0.7619 - Train Acc: 0.7032 - Val Acc: 0.5755\n",
      "Epoch [44/50] - Loss: 0.7540 - Train Acc: 0.7084 - Val Acc: 0.5792\n",
      "Epoch [45/50] - Loss: 0.7822 - Train Acc: 0.6829 - Val Acc: 0.5849\n",
      "Epoch [46/50] - Loss: 0.7624 - Train Acc: 0.6909 - Val Acc: 0.5811\n",
      "Epoch [47/50] - Loss: 0.7827 - Train Acc: 0.6876 - Val Acc: 0.5679\n",
      "Epoch [48/50] - Loss: 0.7656 - Train Acc: 0.7079 - Val Acc: 0.5811\n",
      "Epoch [49/50] - Loss: 0.7610 - Train Acc: 0.7027 - Val Acc: 0.5792\n",
      "Epoch [50/50] - Loss: 0.7556 - Train Acc: 0.6984 - Val Acc: 0.5755\n",
      "\n",
      "===== Fold 4 =====\n",
      "Epoch [1/50] - Loss: 1.7141 - Train Acc: 0.3157 - Val Acc: 0.2717\n",
      "Epoch [2/50] - Loss: 1.5290 - Train Acc: 0.3662 - Val Acc: 0.2925\n",
      "Epoch [3/50] - Loss: 1.4143 - Train Acc: 0.4021 - Val Acc: 0.3547\n",
      "Epoch [4/50] - Loss: 1.4487 - Train Acc: 0.4011 - Val Acc: 0.3774\n",
      "Epoch [5/50] - Loss: 1.3596 - Train Acc: 0.4587 - Val Acc: 0.4887\n",
      "Epoch [6/50] - Loss: 1.2716 - Train Acc: 0.4946 - Val Acc: 0.3943\n",
      "Epoch [7/50] - Loss: 1.2428 - Train Acc: 0.5120 - Val Acc: 0.4679\n",
      "Epoch [8/50] - Loss: 1.3371 - Train Acc: 0.4733 - Val Acc: 0.4453\n",
      "Epoch [9/50] - Loss: 1.2287 - Train Acc: 0.5182 - Val Acc: 0.4660\n",
      "Epoch [10/50] - Loss: 1.2478 - Train Acc: 0.5215 - Val Acc: 0.4094\n",
      "Epoch [11/50] - Loss: 1.1259 - Train Acc: 0.5493 - Val Acc: 0.4981\n",
      "Epoch [12/50] - Loss: 1.1008 - Train Acc: 0.5635 - Val Acc: 0.5245\n",
      "Epoch [13/50] - Loss: 1.0593 - Train Acc: 0.5932 - Val Acc: 0.5151\n",
      "Epoch [14/50] - Loss: 1.0470 - Train Acc: 0.5918 - Val Acc: 0.4943\n",
      "Epoch [15/50] - Loss: 1.0572 - Train Acc: 0.5842 - Val Acc: 0.5132\n",
      "Epoch [16/50] - Loss: 1.0497 - Train Acc: 0.5904 - Val Acc: 0.5000\n",
      "Epoch [17/50] - Loss: 1.0399 - Train Acc: 0.5927 - Val Acc: 0.4887\n",
      "Epoch [18/50] - Loss: 1.0003 - Train Acc: 0.6050 - Val Acc: 0.5151\n",
      "Epoch [19/50] - Loss: 1.0227 - Train Acc: 0.5979 - Val Acc: 0.5226\n",
      "Epoch [20/50] - Loss: 1.0122 - Train Acc: 0.6059 - Val Acc: 0.4906\n",
      "Epoch [21/50] - Loss: 0.9639 - Train Acc: 0.6215 - Val Acc: 0.5000\n",
      "Epoch [22/50] - Loss: 0.9324 - Train Acc: 0.6314 - Val Acc: 0.5094\n",
      "Epoch [23/50] - Loss: 0.9277 - Train Acc: 0.6253 - Val Acc: 0.5075\n",
      "Epoch [24/50] - Loss: 0.9107 - Train Acc: 0.6399 - Val Acc: 0.5038\n",
      "Epoch [25/50] - Loss: 0.9220 - Train Acc: 0.6409 - Val Acc: 0.5226\n",
      "Epoch [26/50] - Loss: 0.9233 - Train Acc: 0.6347 - Val Acc: 0.5132\n",
      "Epoch [27/50] - Loss: 0.9060 - Train Acc: 0.6479 - Val Acc: 0.4962\n",
      "Epoch [28/50] - Loss: 0.9097 - Train Acc: 0.6479 - Val Acc: 0.4981\n",
      "Epoch [29/50] - Loss: 0.8963 - Train Acc: 0.6522 - Val Acc: 0.4830\n",
      "Epoch [30/50] - Loss: 0.8981 - Train Acc: 0.6489 - Val Acc: 0.4736\n",
      "Epoch [31/50] - Loss: 0.8484 - Train Acc: 0.6560 - Val Acc: 0.4962\n",
      "Epoch [32/50] - Loss: 0.8443 - Train Acc: 0.6668 - Val Acc: 0.5245\n",
      "Epoch [33/50] - Loss: 0.8468 - Train Acc: 0.6692 - Val Acc: 0.5170\n",
      "Epoch [34/50] - Loss: 0.8282 - Train Acc: 0.6763 - Val Acc: 0.4830\n",
      "Epoch [35/50] - Loss: 0.8306 - Train Acc: 0.6730 - Val Acc: 0.5075\n",
      "Epoch [36/50] - Loss: 0.8129 - Train Acc: 0.6866 - Val Acc: 0.5038\n",
      "Epoch [37/50] - Loss: 0.8242 - Train Acc: 0.6852 - Val Acc: 0.5113\n",
      "Epoch [38/50] - Loss: 0.8019 - Train Acc: 0.6871 - Val Acc: 0.4925\n",
      "Epoch [39/50] - Loss: 0.7993 - Train Acc: 0.6848 - Val Acc: 0.5208\n",
      "Epoch [40/50] - Loss: 0.7967 - Train Acc: 0.6899 - Val Acc: 0.5094\n",
      "Epoch [41/50] - Loss: 0.7820 - Train Acc: 0.6928 - Val Acc: 0.5340\n",
      "Epoch [42/50] - Loss: 0.7989 - Train Acc: 0.6914 - Val Acc: 0.5170\n",
      "Epoch [43/50] - Loss: 0.7864 - Train Acc: 0.6862 - Val Acc: 0.5377\n",
      "Epoch [44/50] - Loss: 0.7843 - Train Acc: 0.6871 - Val Acc: 0.5283\n",
      "Epoch [45/50] - Loss: 0.7788 - Train Acc: 0.6989 - Val Acc: 0.5434\n",
      "Epoch [46/50] - Loss: 0.7695 - Train Acc: 0.6994 - Val Acc: 0.5358\n",
      "Epoch [47/50] - Loss: 0.7630 - Train Acc: 0.6914 - Val Acc: 0.5358\n",
      "Epoch [48/50] - Loss: 0.7432 - Train Acc: 0.7074 - Val Acc: 0.5245\n",
      "Epoch [49/50] - Loss: 0.7489 - Train Acc: 0.7074 - Val Acc: 0.5358\n",
      "Epoch [50/50] - Loss: 0.7633 - Train Acc: 0.7150 - Val Acc: 0.5566\n",
      "\n",
      "===== Fold 5 =====\n",
      "Epoch [1/50] - Loss: 1.6678 - Train Acc: 0.3203 - Val Acc: 0.3403\n",
      "Epoch [2/50] - Loss: 1.4502 - Train Acc: 0.4132 - Val Acc: 0.3440\n",
      "Epoch [3/50] - Loss: 1.4184 - Train Acc: 0.4212 - Val Acc: 0.3800\n",
      "Epoch [4/50] - Loss: 1.3601 - Train Acc: 0.4472 - Val Acc: 0.4291\n",
      "Epoch [5/50] - Loss: 1.3172 - Train Acc: 0.4736 - Val Acc: 0.4140\n",
      "Epoch [6/50] - Loss: 1.2358 - Train Acc: 0.5071 - Val Acc: 0.4896\n",
      "Epoch [7/50] - Loss: 1.2263 - Train Acc: 0.5108 - Val Acc: 0.4934\n",
      "Epoch [8/50] - Loss: 1.2060 - Train Acc: 0.5259 - Val Acc: 0.4499\n",
      "Epoch [9/50] - Loss: 1.2227 - Train Acc: 0.5146 - Val Acc: 0.5198\n",
      "Epoch [10/50] - Loss: 1.3598 - Train Acc: 0.4618 - Val Acc: 0.4537\n",
      "Epoch [11/50] - Loss: 1.2206 - Train Acc: 0.5019 - Val Acc: 0.4820\n",
      "Epoch [12/50] - Loss: 1.3051 - Train Acc: 0.4759 - Val Acc: 0.5066\n",
      "Epoch [13/50] - Loss: 1.2178 - Train Acc: 0.5156 - Val Acc: 0.5009\n",
      "Epoch [14/50] - Loss: 1.1567 - Train Acc: 0.5448 - Val Acc: 0.5028\n",
      "Epoch [15/50] - Loss: 1.1802 - Train Acc: 0.5340 - Val Acc: 0.4877\n",
      "Epoch [16/50] - Loss: 1.1862 - Train Acc: 0.5292 - Val Acc: 0.4650\n",
      "Epoch [17/50] - Loss: 1.1678 - Train Acc: 0.5443 - Val Acc: 0.5255\n",
      "Epoch [18/50] - Loss: 1.1019 - Train Acc: 0.5679 - Val Acc: 0.5350\n",
      "Epoch [19/50] - Loss: 1.0646 - Train Acc: 0.5774 - Val Acc: 0.5198\n",
      "Epoch [20/50] - Loss: 1.0877 - Train Acc: 0.5759 - Val Acc: 0.5331\n",
      "Epoch [21/50] - Loss: 1.0123 - Train Acc: 0.6028 - Val Acc: 0.5558\n",
      "Epoch [22/50] - Loss: 0.9979 - Train Acc: 0.6090 - Val Acc: 0.5671\n",
      "Epoch [23/50] - Loss: 0.9938 - Train Acc: 0.6014 - Val Acc: 0.5595\n",
      "Epoch [24/50] - Loss: 0.9982 - Train Acc: 0.5991 - Val Acc: 0.5595\n",
      "Epoch [25/50] - Loss: 0.9830 - Train Acc: 0.6208 - Val Acc: 0.5747\n",
      "Epoch [26/50] - Loss: 1.0016 - Train Acc: 0.6108 - Val Acc: 0.5766\n",
      "Epoch [27/50] - Loss: 0.9685 - Train Acc: 0.6203 - Val Acc: 0.5784\n",
      "Epoch [28/50] - Loss: 0.9762 - Train Acc: 0.6104 - Val Acc: 0.5539\n",
      "Epoch [29/50] - Loss: 0.9717 - Train Acc: 0.6113 - Val Acc: 0.5955\n",
      "Epoch [30/50] - Loss: 0.9712 - Train Acc: 0.6222 - Val Acc: 0.5822\n",
      "Epoch [31/50] - Loss: 0.9371 - Train Acc: 0.6349 - Val Acc: 0.5652\n",
      "Epoch [32/50] - Loss: 0.9349 - Train Acc: 0.6382 - Val Acc: 0.5841\n",
      "Epoch [33/50] - Loss: 0.9311 - Train Acc: 0.6368 - Val Acc: 0.5217\n",
      "Epoch [34/50] - Loss: 0.9293 - Train Acc: 0.6340 - Val Acc: 0.5633\n",
      "Epoch [35/50] - Loss: 0.9519 - Train Acc: 0.6283 - Val Acc: 0.5577\n",
      "Epoch [36/50] - Loss: 0.9319 - Train Acc: 0.6344 - Val Acc: 0.5709\n",
      "Epoch [37/50] - Loss: 0.9386 - Train Acc: 0.6160 - Val Acc: 0.5955\n",
      "Epoch [38/50] - Loss: 0.9172 - Train Acc: 0.6439 - Val Acc: 0.5841\n",
      "Epoch [39/50] - Loss: 0.9126 - Train Acc: 0.6472 - Val Acc: 0.5917\n",
      "Epoch [40/50] - Loss: 0.8856 - Train Acc: 0.6462 - Val Acc: 0.5974\n",
      "Epoch [41/50] - Loss: 0.8706 - Train Acc: 0.6561 - Val Acc: 0.5879\n",
      "Epoch [42/50] - Loss: 0.8855 - Train Acc: 0.6575 - Val Acc: 0.5917\n",
      "Epoch [43/50] - Loss: 0.8705 - Train Acc: 0.6561 - Val Acc: 0.5860\n",
      "Epoch [44/50] - Loss: 0.8779 - Train Acc: 0.6580 - Val Acc: 0.5955\n",
      "Epoch [45/50] - Loss: 0.8597 - Train Acc: 0.6660 - Val Acc: 0.6011\n",
      "Epoch [46/50] - Loss: 0.8841 - Train Acc: 0.6495 - Val Acc: 0.5955\n",
      "Epoch [47/50] - Loss: 0.8632 - Train Acc: 0.6660 - Val Acc: 0.6068\n",
      "Epoch [48/50] - Loss: 0.8744 - Train Acc: 0.6684 - Val Acc: 0.6030\n",
      "Epoch [49/50] - Loss: 0.8704 - Train Acc: 0.6585 - Val Acc: 0.5822\n",
      "Epoch [50/50] - Loss: 0.8540 - Train Acc: 0.6613 - Val Acc: 0.5992\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load preprocessed data\n",
    "X = np.load(\"X_sequences.npy\", allow_pickle=True)\n",
    "y = np.load(\"y_labels.npy\", allow_pickle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(type(X))          # Likely <class 'list'>\n",
    "print(type(X[0]))\n",
    "print(device)        # Works only if X[0] is ndarray\n",
    "\n",
    "X = [np.array(seq) for seq in X]\n",
    "num_features = X[0].shape[1]\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "    X = np.array([np.array(seq) for seq in X], dtype=object)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Now this works:\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # X_train, X_val = X[train_idx], X[val_idx]\n",
    "    # y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    train_dataset = PoseSequenceDataset(X_train, y_train)\n",
    "    val_dataset = PoseSequenceDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = PoseSequenceTransformer(num_features=num_features, num_classes=num_classes).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(50):  # could be 100\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_targets = [], []\n",
    "\n",
    "        for seqs, labels in train_loader:\n",
    "            seqs, labels = seqs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(seqs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            train_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for seqs, labels in val_loader:\n",
    "                seqs, labels = seqs.to(device), labels.to(device)\n",
    "                outputs = model(seqs)\n",
    "                val_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "                val_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_targets, val_preds)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/50] - Loss: {train_loss/len(train_loader):.4f} - Train Acc: {train_acc:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    fold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e7b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"pose_transformer.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Fall Down:   0%|          | 0/177 [00:00<?, ?it/s]C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_32584\\3711796478.py:39: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pred_x, pred_y = int(kf.x[0]), int(kf.x[1])\n",
      "Processing Fall Down: 100%|██████████| 177/177 [03:19<00:00,  1.13s/it]\n",
      "Processing Lying Down: 100%|██████████| 162/162 [04:13<00:00,  1.56s/it]\n",
      "Processing Sit down: 100%|██████████| 79/79 [01:24<00:00,  1.07s/it]\n",
      "Processing Sitting: 100%|██████████| 135/135 [05:01<00:00,  2.24s/it]\n",
      "Processing Stand up: 100%|██████████| 126/126 [02:22<00:00,  1.13s/it]\n",
      "Processing Standing: 100%|██████████| 242/242 [07:46<00:00,  1.93s/it]\n",
      "Processing Walking: 100%|██████████| 285/285 [10:42<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1139 test sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== Preprocess test videos =====\n",
    "TEST_PATH = r\"test\"  # path to your test folder\n",
    "actions_test = os.listdir(TEST_PATH)\n",
    "\n",
    "pose_kalman_filters = {}\n",
    "joint_history = {}\n",
    "angle_history = {}\n",
    "status_history = {}\n",
    "\n",
    "test_sequences = []\n",
    "test_labels = []\n",
    "\n",
    "for action in actions_test:\n",
    "    action_path = os.path.join(TEST_PATH, action)\n",
    "    videos = [f for f in os.listdir(action_path) if f.lower().endswith(('.mp4', '.avi'))]\n",
    "\n",
    "    for video_file in tqdm(videos, desc=f\"Processing {action}\"):\n",
    "        video_path = os.path.join(action_path, video_file)\n",
    "        frames = preprocess_video(video_path, background_subtraction=False)\n",
    "\n",
    "        sequence_features = []\n",
    "        with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "            for frame in frames:\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = pose.process(image)\n",
    "\n",
    "                if results.pose_landmarks:\n",
    "                    frame_features = []\n",
    "                    for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "                        x, y = int(lm.x * frame.shape[1]), int(lm.y * frame.shape[0])\n",
    "                        key = f'joint_{idx}'\n",
    "\n",
    "                        if key not in pose_kalman_filters:\n",
    "                            pose_kalman_filters[key] = create_kalman_filter()\n",
    "                            pose_kalman_filters[key].x[:2] = np.array([[x], [y]])\n",
    "                        kf = pose_kalman_filters[key]\n",
    "                        kf.predict()\n",
    "                        kf.update(np.array([x, y]))\n",
    "                        pred_x, pred_y = int(kf.x[0]), int(kf.x[1])\n",
    "                        frame_features.extend([pred_x, pred_y])\n",
    "\n",
    "                    for name, (a_idx, b_idx, c_idx) in ANGLE_JOINTS.items():\n",
    "                        if all(joint_history.get(f'joint_{i}', []) for i in (a_idx, b_idx, c_idx)):\n",
    "                            a = (pose_kalman_filters[f'joint_{a_idx}'].x[0], pose_kalman_filters[f'joint_{a_idx}'].x[1])\n",
    "                            b = (pose_kalman_filters[f'joint_{b_idx}'].x[0], pose_kalman_filters[f'joint_{b_idx}'].x[1])\n",
    "                            c = (pose_kalman_filters[f'joint_{c_idx}'].x[0], pose_kalman_filters[f'joint_{c_idx}'].x[1])\n",
    "\n",
    "                            angle = calculate_angle(a, b, c)\n",
    "                            delta_y = b[1] - joint_history.get(f'joint_{b_idx}', [(0,0)])[-1][1]\n",
    "                            angle_velocity = 0\n",
    "                            if name in angle_history and len(angle_history[name]) > 0:\n",
    "                                angle_velocity = angle - angle_history[name][-1]\n",
    "\n",
    "                            label = classify_joint_movement(delta_y, angle_velocity)\n",
    "                            frame_features.extend([angle, status_history.get(name, [\"STATIC\"])[-1] if name in status_history else \"STATIC\"])\n",
    "                        else:\n",
    "                            frame_features.extend([0, \"STATIC\"])\n",
    "\n",
    "                    status_map = {\"UP\": 1, \"DOWN\": -1, \"STATIC\": 0}\n",
    "                    frame_features = [status_map.get(f, f) if isinstance(f, str) else f for f in frame_features]\n",
    "                    sequence_features.append(frame_features)\n",
    "\n",
    "        if sequence_features:\n",
    "            test_sequences.append(sequence_features)\n",
    "            test_labels.append(action)\n",
    "\n",
    "# Save test arrays\n",
    "np.save(\"X_test.npy\", np.array(test_sequences, dtype=object))\n",
    "np.save(\"y_test.npy\", np.array(test_labels))\n",
    "print(f\"Saved {len(test_sequences)} test sequences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f655c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoseSequenceTransformer(\n",
       "  (input_fc): Linear(in_features=78, out_features=128, bias=True)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=128, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load processed test data\n",
    "X_test = np.load(\"X_test.npy\", allow_pickle=True)\n",
    "y_test = np.load(\"y_test.npy\", allow_pickle=True)\n",
    "\n",
    "# Match label encoding with training\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.load(\"y_labels.npy\", allow_pickle=True))  # fit on train labels\n",
    "\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "X_test = [np.array(seq) for seq in X_test]\n",
    "\n",
    "# Prepare DataLoader\n",
    "test_dataset = PoseSequenceDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_features = X_test[0].shape[1]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "model = PoseSequenceTransformer(num_features=num_features, num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"pose_transformer.pth\"))  \n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f3798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5733\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Fall Down       0.66      0.53      0.59       169\n",
      "  Lying Down       0.62      0.83      0.71       142\n",
      "    Sit down       0.43      0.26      0.32        77\n",
      "     Sitting       0.33      0.35      0.34       120\n",
      "    Stand up       0.47      0.39      0.43       119\n",
      "    Standing       0.54      0.66      0.59       229\n",
      "     Walking       0.73      0.65      0.69       283\n",
      "\n",
      "    accuracy                           0.57      1139\n",
      "   macro avg       0.54      0.53      0.52      1139\n",
      "weighted avg       0.58      0.57      0.57      1139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "all_preds, all_targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seqs, labels in test_loader:\n",
    "        seqs, labels = seqs.to(device), labels.to(device)\n",
    "        outputs = model(seqs)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e874a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "all_preds, all_targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seqs, labels in test_loader:\n",
    "        seqs, labels = seqs.to(device), labels.to(device)\n",
    "        outputs = model(seqs)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472d0687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078c3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "cuda\n",
      "\n",
      "===== Fold 1 =====\n",
      "Epoch [1/80] - Loss: 1.7588 - Train Acc: 0.2827 - Val Acc: 0.3283\n",
      "Epoch [2/80] - Loss: 1.5352 - Train Acc: 0.3709 - Val Acc: 0.3075\n",
      "Epoch [3/80] - Loss: 1.4797 - Train Acc: 0.3893 - Val Acc: 0.3585\n",
      "Epoch [4/80] - Loss: 1.4358 - Train Acc: 0.4210 - Val Acc: 0.3434\n",
      "Epoch [5/80] - Loss: 1.4128 - Train Acc: 0.4422 - Val Acc: 0.4057\n",
      "Epoch [6/80] - Loss: 1.3178 - Train Acc: 0.4700 - Val Acc: 0.4226\n",
      "Epoch [7/80] - Loss: 1.2923 - Train Acc: 0.4733 - Val Acc: 0.3811\n",
      "Epoch [8/80] - Loss: 1.2763 - Train Acc: 0.4889 - Val Acc: 0.4019\n",
      "Epoch [9/80] - Loss: 1.3074 - Train Acc: 0.4757 - Val Acc: 0.4075\n",
      "Epoch [10/80] - Loss: 1.2976 - Train Acc: 0.4799 - Val Acc: 0.4113\n",
      "Epoch [11/80] - Loss: 1.2119 - Train Acc: 0.5229 - Val Acc: 0.4491\n",
      "Epoch [12/80] - Loss: 1.1785 - Train Acc: 0.5503 - Val Acc: 0.5000\n",
      "Epoch [13/80] - Loss: 1.1447 - Train Acc: 0.5521 - Val Acc: 0.5019\n",
      "Epoch [14/80] - Loss: 1.1405 - Train Acc: 0.5493 - Val Acc: 0.4906\n",
      "Epoch [15/80] - Loss: 1.1256 - Train Acc: 0.5583 - Val Acc: 0.4943\n",
      "Epoch [16/80] - Loss: 1.0972 - Train Acc: 0.5748 - Val Acc: 0.5094\n",
      "Epoch [17/80] - Loss: 1.0749 - Train Acc: 0.5805 - Val Acc: 0.4943\n",
      "Epoch [18/80] - Loss: 1.1062 - Train Acc: 0.5706 - Val Acc: 0.4981\n",
      "Epoch [19/80] - Loss: 1.1025 - Train Acc: 0.5677 - Val Acc: 0.4811\n",
      "Epoch [20/80] - Loss: 1.1353 - Train Acc: 0.5578 - Val Acc: 0.5000\n",
      "Epoch [21/80] - Loss: 1.0595 - Train Acc: 0.5894 - Val Acc: 0.5226\n",
      "Epoch [22/80] - Loss: 0.9962 - Train Acc: 0.6012 - Val Acc: 0.5340\n",
      "Epoch [23/80] - Loss: 1.0101 - Train Acc: 0.5989 - Val Acc: 0.5396\n",
      "Epoch [24/80] - Loss: 0.9892 - Train Acc: 0.6192 - Val Acc: 0.5547\n",
      "Epoch [25/80] - Loss: 0.9702 - Train Acc: 0.6187 - Val Acc: 0.5132\n",
      "Epoch [26/80] - Loss: 0.9905 - Train Acc: 0.6149 - Val Acc: 0.5396\n",
      "Epoch [27/80] - Loss: 0.9817 - Train Acc: 0.6225 - Val Acc: 0.5491\n",
      "Epoch [28/80] - Loss: 0.9735 - Train Acc: 0.6234 - Val Acc: 0.5415\n",
      "Epoch [29/80] - Loss: 0.9570 - Train Acc: 0.6291 - Val Acc: 0.5340\n",
      "Epoch [30/80] - Loss: 0.9765 - Train Acc: 0.6074 - Val Acc: 0.5358\n",
      "Epoch [31/80] - Loss: 0.9422 - Train Acc: 0.6484 - Val Acc: 0.5434\n",
      "Epoch [32/80] - Loss: 0.9090 - Train Acc: 0.6451 - Val Acc: 0.5434\n",
      "Epoch [33/80] - Loss: 0.9124 - Train Acc: 0.6376 - Val Acc: 0.5189\n",
      "Epoch [34/80] - Loss: 0.8929 - Train Acc: 0.6465 - Val Acc: 0.5302\n",
      "Epoch [35/80] - Loss: 0.9020 - Train Acc: 0.6527 - Val Acc: 0.5283\n",
      "Epoch [36/80] - Loss: 0.8991 - Train Acc: 0.6541 - Val Acc: 0.5415\n",
      "Epoch [37/80] - Loss: 0.8945 - Train Acc: 0.6517 - Val Acc: 0.5623\n",
      "Epoch [38/80] - Loss: 0.8807 - Train Acc: 0.6503 - Val Acc: 0.5321\n",
      "Epoch [39/80] - Loss: 0.8888 - Train Acc: 0.6517 - Val Acc: 0.5472\n",
      "Epoch [40/80] - Loss: 0.8692 - Train Acc: 0.6531 - Val Acc: 0.5528\n",
      "Epoch [41/80] - Loss: 0.8759 - Train Acc: 0.6692 - Val Acc: 0.5415\n",
      "Epoch [42/80] - Loss: 0.8730 - Train Acc: 0.6711 - Val Acc: 0.5472\n",
      "Epoch [43/80] - Loss: 0.8668 - Train Acc: 0.6664 - Val Acc: 0.5453\n",
      "Epoch [44/80] - Loss: 0.8608 - Train Acc: 0.6654 - Val Acc: 0.5358\n",
      "Epoch [45/80] - Loss: 0.8653 - Train Acc: 0.6687 - Val Acc: 0.5358\n",
      "Epoch [46/80] - Loss: 0.8565 - Train Acc: 0.6649 - Val Acc: 0.5396\n",
      "Epoch [47/80] - Loss: 0.8610 - Train Acc: 0.6602 - Val Acc: 0.5491\n",
      "Epoch [48/80] - Loss: 0.8490 - Train Acc: 0.6673 - Val Acc: 0.5623\n",
      "Epoch [49/80] - Loss: 0.8533 - Train Acc: 0.6630 - Val Acc: 0.5509\n",
      "Epoch [50/80] - Loss: 0.8441 - Train Acc: 0.6701 - Val Acc: 0.5604\n",
      "Epoch [51/80] - Loss: 0.8140 - Train Acc: 0.6782 - Val Acc: 0.5660\n",
      "Epoch [52/80] - Loss: 0.8399 - Train Acc: 0.6697 - Val Acc: 0.5623\n",
      "Epoch [53/80] - Loss: 0.8382 - Train Acc: 0.6715 - Val Acc: 0.5604\n",
      "Epoch [54/80] - Loss: 0.8202 - Train Acc: 0.6857 - Val Acc: 0.5604\n",
      "Epoch [55/80] - Loss: 0.8239 - Train Acc: 0.6739 - Val Acc: 0.5642\n",
      "Epoch [56/80] - Loss: 0.8168 - Train Acc: 0.6805 - Val Acc: 0.5623\n",
      "Epoch [57/80] - Loss: 0.8260 - Train Acc: 0.6848 - Val Acc: 0.5547\n",
      "Epoch [58/80] - Loss: 0.8234 - Train Acc: 0.6786 - Val Acc: 0.5623\n",
      "Epoch [59/80] - Loss: 0.8165 - Train Acc: 0.6800 - Val Acc: 0.5698\n",
      "Epoch [60/80] - Loss: 0.7974 - Train Acc: 0.6871 - Val Acc: 0.5566\n",
      "Epoch [61/80] - Loss: 0.8176 - Train Acc: 0.6767 - Val Acc: 0.5604\n",
      "Epoch [62/80] - Loss: 0.8047 - Train Acc: 0.6866 - Val Acc: 0.5623\n",
      "Epoch [63/80] - Loss: 0.7948 - Train Acc: 0.6871 - Val Acc: 0.5547\n",
      "Epoch [64/80] - Loss: 0.7980 - Train Acc: 0.6989 - Val Acc: 0.5528\n",
      "Epoch [65/80] - Loss: 0.8275 - Train Acc: 0.6857 - Val Acc: 0.5547\n",
      "Epoch [66/80] - Loss: 0.8065 - Train Acc: 0.6810 - Val Acc: 0.5585\n",
      "Epoch [67/80] - Loss: 0.8069 - Train Acc: 0.6862 - Val Acc: 0.5566\n",
      "Epoch [68/80] - Loss: 0.7993 - Train Acc: 0.6966 - Val Acc: 0.5566\n",
      "Epoch [69/80] - Loss: 0.7943 - Train Acc: 0.6923 - Val Acc: 0.5547\n",
      "Epoch [70/80] - Loss: 0.8022 - Train Acc: 0.6904 - Val Acc: 0.5566\n",
      "Epoch [71/80] - Loss: 0.8197 - Train Acc: 0.6843 - Val Acc: 0.5566\n",
      "Epoch [72/80] - Loss: 0.8058 - Train Acc: 0.6848 - Val Acc: 0.5585\n",
      "Epoch [73/80] - Loss: 0.8045 - Train Acc: 0.6796 - Val Acc: 0.5547\n",
      "Epoch [74/80] - Loss: 0.8068 - Train Acc: 0.6843 - Val Acc: 0.5566\n",
      "Epoch [75/80] - Loss: 0.7955 - Train Acc: 0.6824 - Val Acc: 0.5547\n",
      "Epoch [76/80] - Loss: 0.7947 - Train Acc: 0.6966 - Val Acc: 0.5528\n",
      "Epoch [77/80] - Loss: 0.8183 - Train Acc: 0.6848 - Val Acc: 0.5566\n",
      "Epoch [78/80] - Loss: 0.8401 - Train Acc: 0.6876 - Val Acc: 0.5585\n",
      "Epoch [79/80] - Loss: 0.8170 - Train Acc: 0.6763 - Val Acc: 0.5547\n",
      "Epoch [80/80] - Loss: 0.8054 - Train Acc: 0.6899 - Val Acc: 0.5547\n",
      "\n",
      "===== Fold 2 =====\n",
      "Epoch [1/80] - Loss: 1.6786 - Train Acc: 0.3346 - Val Acc: 0.2792\n",
      "Epoch [2/80] - Loss: 1.5577 - Train Acc: 0.3407 - Val Acc: 0.3453\n",
      "Epoch [3/80] - Loss: 1.4550 - Train Acc: 0.3841 - Val Acc: 0.3660\n",
      "Epoch [4/80] - Loss: 1.4143 - Train Acc: 0.4181 - Val Acc: 0.3906\n",
      "Epoch [5/80] - Loss: 1.3600 - Train Acc: 0.4559 - Val Acc: 0.3528\n",
      "Epoch [6/80] - Loss: 1.3909 - Train Acc: 0.4479 - Val Acc: 0.3698\n",
      "Epoch [7/80] - Loss: 1.3695 - Train Acc: 0.4469 - Val Acc: 0.4623\n",
      "Epoch [8/80] - Loss: 1.3361 - Train Acc: 0.4625 - Val Acc: 0.4604\n",
      "Epoch [9/80] - Loss: 1.2969 - Train Acc: 0.4955 - Val Acc: 0.4774\n",
      "Epoch [10/80] - Loss: 1.2635 - Train Acc: 0.4795 - Val Acc: 0.4755\n",
      "Epoch [11/80] - Loss: 1.1715 - Train Acc: 0.5300 - Val Acc: 0.5377\n",
      "Epoch [12/80] - Loss: 1.1355 - Train Acc: 0.5611 - Val Acc: 0.5491\n",
      "Epoch [13/80] - Loss: 1.1154 - Train Acc: 0.5550 - Val Acc: 0.5547\n",
      "Epoch [14/80] - Loss: 1.0885 - Train Acc: 0.5739 - Val Acc: 0.5774\n",
      "Epoch [15/80] - Loss: 1.0922 - Train Acc: 0.5677 - Val Acc: 0.5755\n",
      "Epoch [16/80] - Loss: 1.0692 - Train Acc: 0.5824 - Val Acc: 0.5698\n",
      "Epoch [17/80] - Loss: 1.0640 - Train Acc: 0.5852 - Val Acc: 0.5717\n",
      "Epoch [18/80] - Loss: 1.0453 - Train Acc: 0.5913 - Val Acc: 0.5925\n",
      "Epoch [19/80] - Loss: 1.0463 - Train Acc: 0.5927 - Val Acc: 0.5830\n",
      "Epoch [20/80] - Loss: 1.0186 - Train Acc: 0.6107 - Val Acc: 0.5736\n",
      "Epoch [21/80] - Loss: 0.9749 - Train Acc: 0.6177 - Val Acc: 0.5774\n",
      "Epoch [22/80] - Loss: 0.9509 - Train Acc: 0.6376 - Val Acc: 0.5887\n",
      "Epoch [23/80] - Loss: 0.9516 - Train Acc: 0.6267 - Val Acc: 0.5906\n",
      "Epoch [24/80] - Loss: 0.9274 - Train Acc: 0.6281 - Val Acc: 0.6113\n",
      "Epoch [25/80] - Loss: 0.9332 - Train Acc: 0.6423 - Val Acc: 0.5774\n",
      "Epoch [26/80] - Loss: 0.9399 - Train Acc: 0.6361 - Val Acc: 0.6057\n",
      "Epoch [27/80] - Loss: 0.9295 - Train Acc: 0.6310 - Val Acc: 0.6302\n",
      "Epoch [28/80] - Loss: 0.9278 - Train Acc: 0.6390 - Val Acc: 0.6075\n",
      "Epoch [29/80] - Loss: 0.9105 - Train Acc: 0.6503 - Val Acc: 0.6075\n",
      "Epoch [30/80] - Loss: 0.9005 - Train Acc: 0.6564 - Val Acc: 0.5943\n",
      "Epoch [31/80] - Loss: 0.8691 - Train Acc: 0.6607 - Val Acc: 0.6321\n",
      "Epoch [32/80] - Loss: 0.8391 - Train Acc: 0.6786 - Val Acc: 0.6302\n",
      "Epoch [33/80] - Loss: 0.8523 - Train Acc: 0.6753 - Val Acc: 0.6264\n",
      "Epoch [34/80] - Loss: 0.8436 - Train Acc: 0.6739 - Val Acc: 0.6245\n",
      "Epoch [35/80] - Loss: 0.8424 - Train Acc: 0.6701 - Val Acc: 0.6170\n",
      "Epoch [36/80] - Loss: 0.8654 - Train Acc: 0.6753 - Val Acc: 0.6302\n",
      "Epoch [37/80] - Loss: 0.8304 - Train Acc: 0.6871 - Val Acc: 0.6377\n",
      "Epoch [38/80] - Loss: 0.8188 - Train Acc: 0.6824 - Val Acc: 0.6453\n",
      "Epoch [39/80] - Loss: 0.8122 - Train Acc: 0.6876 - Val Acc: 0.6208\n",
      "Epoch [40/80] - Loss: 0.8211 - Train Acc: 0.6848 - Val Acc: 0.6528\n",
      "Epoch [41/80] - Loss: 0.7905 - Train Acc: 0.6918 - Val Acc: 0.6377\n",
      "Epoch [42/80] - Loss: 0.7943 - Train Acc: 0.6928 - Val Acc: 0.6491\n",
      "Epoch [43/80] - Loss: 0.7921 - Train Acc: 0.6999 - Val Acc: 0.6340\n",
      "Epoch [44/80] - Loss: 0.7812 - Train Acc: 0.6975 - Val Acc: 0.6491\n",
      "Epoch [45/80] - Loss: 0.8122 - Train Acc: 0.7013 - Val Acc: 0.6321\n",
      "Epoch [46/80] - Loss: 0.7632 - Train Acc: 0.7032 - Val Acc: 0.6415\n",
      "Epoch [47/80] - Loss: 0.7552 - Train Acc: 0.7112 - Val Acc: 0.6283\n",
      "Epoch [48/80] - Loss: 0.7972 - Train Acc: 0.6923 - Val Acc: 0.6358\n",
      "Epoch [49/80] - Loss: 0.7725 - Train Acc: 0.6999 - Val Acc: 0.6340\n",
      "Epoch [50/80] - Loss: 0.7638 - Train Acc: 0.7046 - Val Acc: 0.6453\n",
      "Epoch [51/80] - Loss: 0.7453 - Train Acc: 0.6994 - Val Acc: 0.6415\n",
      "Epoch [52/80] - Loss: 0.7689 - Train Acc: 0.7022 - Val Acc: 0.6396\n",
      "Epoch [53/80] - Loss: 0.7416 - Train Acc: 0.7065 - Val Acc: 0.6377\n",
      "Epoch [54/80] - Loss: 0.7316 - Train Acc: 0.7187 - Val Acc: 0.6283\n",
      "Epoch [55/80] - Loss: 0.7317 - Train Acc: 0.7187 - Val Acc: 0.6358\n",
      "Epoch [56/80] - Loss: 0.7485 - Train Acc: 0.7008 - Val Acc: 0.6415\n",
      "Epoch [57/80] - Loss: 0.7437 - Train Acc: 0.7197 - Val Acc: 0.6340\n",
      "Epoch [58/80] - Loss: 0.7367 - Train Acc: 0.7131 - Val Acc: 0.6396\n",
      "Epoch [59/80] - Loss: 0.7323 - Train Acc: 0.7098 - Val Acc: 0.6321\n",
      "Epoch [60/80] - Loss: 0.7238 - Train Acc: 0.7263 - Val Acc: 0.6226\n",
      "Epoch [61/80] - Loss: 0.7162 - Train Acc: 0.7211 - Val Acc: 0.6208\n",
      "Epoch [62/80] - Loss: 0.7140 - Train Acc: 0.7249 - Val Acc: 0.6283\n",
      "Epoch [63/80] - Loss: 0.7408 - Train Acc: 0.7079 - Val Acc: 0.6283\n",
      "Epoch [64/80] - Loss: 0.7151 - Train Acc: 0.7329 - Val Acc: 0.6264\n",
      "Epoch [65/80] - Loss: 0.7225 - Train Acc: 0.7291 - Val Acc: 0.6340\n",
      "Epoch [66/80] - Loss: 0.7067 - Train Acc: 0.7197 - Val Acc: 0.6245\n",
      "Epoch [67/80] - Loss: 0.7192 - Train Acc: 0.7220 - Val Acc: 0.6283\n",
      "Epoch [68/80] - Loss: 0.7194 - Train Acc: 0.7244 - Val Acc: 0.6302\n",
      "Epoch [69/80] - Loss: 0.7254 - Train Acc: 0.7239 - Val Acc: 0.6321\n",
      "Epoch [70/80] - Loss: 0.7135 - Train Acc: 0.7296 - Val Acc: 0.6264\n",
      "Epoch [71/80] - Loss: 0.7067 - Train Acc: 0.7230 - Val Acc: 0.6264\n",
      "Epoch [72/80] - Loss: 0.7281 - Train Acc: 0.7145 - Val Acc: 0.6264\n",
      "Epoch [73/80] - Loss: 0.7061 - Train Acc: 0.7263 - Val Acc: 0.6321\n",
      "Epoch [74/80] - Loss: 0.7072 - Train Acc: 0.7197 - Val Acc: 0.6340\n",
      "Epoch [75/80] - Loss: 0.6999 - Train Acc: 0.7282 - Val Acc: 0.6302\n",
      "Epoch [76/80] - Loss: 0.7128 - Train Acc: 0.7197 - Val Acc: 0.6264\n",
      "Epoch [77/80] - Loss: 0.7186 - Train Acc: 0.7187 - Val Acc: 0.6283\n",
      "Epoch [78/80] - Loss: 0.7167 - Train Acc: 0.7187 - Val Acc: 0.6321\n",
      "Epoch [79/80] - Loss: 0.7083 - Train Acc: 0.7357 - Val Acc: 0.6283\n",
      "Epoch [80/80] - Loss: 0.7031 - Train Acc: 0.7258 - Val Acc: 0.6358\n",
      "\n",
      "===== Fold 3 =====\n",
      "Epoch [1/80] - Loss: 1.7374 - Train Acc: 0.3110 - Val Acc: 0.2415\n",
      "Epoch [2/80] - Loss: 1.5028 - Train Acc: 0.3785 - Val Acc: 0.3000\n",
      "Epoch [3/80] - Loss: 1.4547 - Train Acc: 0.3804 - Val Acc: 0.3340\n",
      "Epoch [4/80] - Loss: 1.4536 - Train Acc: 0.3898 - Val Acc: 0.3113\n",
      "Epoch [5/80] - Loss: 1.3966 - Train Acc: 0.4285 - Val Acc: 0.3377\n",
      "Epoch [6/80] - Loss: 1.3821 - Train Acc: 0.4469 - Val Acc: 0.3509\n",
      "Epoch [7/80] - Loss: 1.3623 - Train Acc: 0.4597 - Val Acc: 0.4000\n",
      "Epoch [8/80] - Loss: 1.3530 - Train Acc: 0.4535 - Val Acc: 0.3868\n",
      "Epoch [9/80] - Loss: 1.3022 - Train Acc: 0.4866 - Val Acc: 0.3868\n",
      "Epoch [10/80] - Loss: 1.3019 - Train Acc: 0.4880 - Val Acc: 0.4132\n",
      "Epoch [11/80] - Loss: 1.2436 - Train Acc: 0.5186 - Val Acc: 0.3868\n",
      "Epoch [12/80] - Loss: 1.1942 - Train Acc: 0.5243 - Val Acc: 0.4321\n",
      "Epoch [13/80] - Loss: 1.1522 - Train Acc: 0.5493 - Val Acc: 0.4585\n",
      "Epoch [14/80] - Loss: 1.1308 - Train Acc: 0.5583 - Val Acc: 0.4283\n",
      "Epoch [15/80] - Loss: 1.1347 - Train Acc: 0.5696 - Val Acc: 0.4321\n",
      "Epoch [16/80] - Loss: 1.1101 - Train Acc: 0.5739 - Val Acc: 0.4491\n",
      "Epoch [17/80] - Loss: 1.1426 - Train Acc: 0.5540 - Val Acc: 0.4453\n",
      "Epoch [18/80] - Loss: 1.1385 - Train Acc: 0.5564 - Val Acc: 0.5170\n",
      "Epoch [19/80] - Loss: 1.1153 - Train Acc: 0.5715 - Val Acc: 0.4623\n",
      "Epoch [20/80] - Loss: 1.0856 - Train Acc: 0.5809 - Val Acc: 0.4962\n",
      "Epoch [21/80] - Loss: 1.0518 - Train Acc: 0.5899 - Val Acc: 0.4925\n",
      "Epoch [22/80] - Loss: 1.0404 - Train Acc: 0.5975 - Val Acc: 0.5000\n",
      "Epoch [23/80] - Loss: 1.0095 - Train Acc: 0.6064 - Val Acc: 0.5434\n",
      "Epoch [24/80] - Loss: 1.0342 - Train Acc: 0.5960 - Val Acc: 0.5151\n",
      "Epoch [25/80] - Loss: 0.9904 - Train Acc: 0.6225 - Val Acc: 0.4962\n",
      "Epoch [26/80] - Loss: 0.9856 - Train Acc: 0.6130 - Val Acc: 0.5038\n",
      "Epoch [27/80] - Loss: 0.9754 - Train Acc: 0.6130 - Val Acc: 0.5453\n",
      "Epoch [28/80] - Loss: 0.9635 - Train Acc: 0.6272 - Val Acc: 0.5302\n",
      "Epoch [29/80] - Loss: 0.9976 - Train Acc: 0.6116 - Val Acc: 0.4962\n",
      "Epoch [30/80] - Loss: 1.0209 - Train Acc: 0.5941 - Val Acc: 0.5377\n",
      "Epoch [31/80] - Loss: 0.9646 - Train Acc: 0.6262 - Val Acc: 0.5340\n",
      "Epoch [32/80] - Loss: 0.9634 - Train Acc: 0.6229 - Val Acc: 0.5132\n",
      "Epoch [33/80] - Loss: 0.9623 - Train Acc: 0.6319 - Val Acc: 0.5547\n",
      "Epoch [34/80] - Loss: 0.9520 - Train Acc: 0.6262 - Val Acc: 0.5283\n",
      "Epoch [35/80] - Loss: 0.9585 - Train Acc: 0.6324 - Val Acc: 0.5491\n",
      "Epoch [36/80] - Loss: 0.9444 - Train Acc: 0.6517 - Val Acc: 0.5170\n",
      "Epoch [37/80] - Loss: 0.9356 - Train Acc: 0.6352 - Val Acc: 0.5321\n",
      "Epoch [38/80] - Loss: 0.9502 - Train Acc: 0.6338 - Val Acc: 0.5509\n",
      "Epoch [39/80] - Loss: 0.9362 - Train Acc: 0.6366 - Val Acc: 0.5377\n",
      "Epoch [40/80] - Loss: 0.9327 - Train Acc: 0.6376 - Val Acc: 0.5377\n",
      "Epoch [41/80] - Loss: 0.9151 - Train Acc: 0.6390 - Val Acc: 0.5340\n",
      "Epoch [42/80] - Loss: 0.8825 - Train Acc: 0.6513 - Val Acc: 0.5302\n",
      "Epoch [43/80] - Loss: 0.8685 - Train Acc: 0.6597 - Val Acc: 0.5434\n",
      "Epoch [44/80] - Loss: 0.8895 - Train Acc: 0.6513 - Val Acc: 0.5321\n",
      "Epoch [45/80] - Loss: 0.8808 - Train Acc: 0.6451 - Val Acc: 0.5377\n",
      "Epoch [46/80] - Loss: 0.9026 - Train Acc: 0.6531 - Val Acc: 0.5302\n",
      "Epoch [47/80] - Loss: 0.8893 - Train Acc: 0.6475 - Val Acc: 0.5358\n",
      "Epoch [48/80] - Loss: 0.9337 - Train Acc: 0.6413 - Val Acc: 0.5434\n",
      "Epoch [49/80] - Loss: 0.8861 - Train Acc: 0.6513 - Val Acc: 0.5415\n",
      "Epoch [50/80] - Loss: 0.8980 - Train Acc: 0.6541 - Val Acc: 0.5226\n",
      "Epoch [51/80] - Loss: 0.8840 - Train Acc: 0.6593 - Val Acc: 0.5245\n",
      "Epoch [52/80] - Loss: 0.9038 - Train Acc: 0.6522 - Val Acc: 0.5189\n",
      "Epoch [53/80] - Loss: 0.8955 - Train Acc: 0.6465 - Val Acc: 0.5302\n",
      "Epoch [54/80] - Loss: 0.9183 - Train Acc: 0.6399 - Val Acc: 0.5340\n",
      "Epoch [55/80] - Loss: 0.9020 - Train Acc: 0.6508 - Val Acc: 0.5302\n",
      "Epoch [56/80] - Loss: 0.8879 - Train Acc: 0.6517 - Val Acc: 0.5245\n",
      "Epoch [57/80] - Loss: 0.9044 - Train Acc: 0.6390 - Val Acc: 0.5226\n",
      "Epoch [58/80] - Loss: 0.9014 - Train Acc: 0.6588 - Val Acc: 0.5302\n",
      "Epoch [59/80] - Loss: 0.8702 - Train Acc: 0.6522 - Val Acc: 0.5264\n",
      "Epoch [60/80] - Loss: 0.9430 - Train Acc: 0.6347 - Val Acc: 0.4925\n",
      "Epoch [61/80] - Loss: 0.9299 - Train Acc: 0.6380 - Val Acc: 0.5075\n",
      "Epoch [62/80] - Loss: 0.8955 - Train Acc: 0.6527 - Val Acc: 0.5019\n",
      "Epoch [63/80] - Loss: 0.9039 - Train Acc: 0.6527 - Val Acc: 0.5113\n",
      "Epoch [64/80] - Loss: 0.8979 - Train Acc: 0.6503 - Val Acc: 0.5094\n",
      "Epoch [65/80] - Loss: 0.8894 - Train Acc: 0.6503 - Val Acc: 0.5113\n",
      "Epoch [66/80] - Loss: 0.8914 - Train Acc: 0.6494 - Val Acc: 0.5151\n",
      "Epoch [67/80] - Loss: 0.8985 - Train Acc: 0.6432 - Val Acc: 0.5132\n",
      "Epoch [68/80] - Loss: 0.8981 - Train Acc: 0.6399 - Val Acc: 0.5038\n",
      "Epoch [69/80] - Loss: 0.8953 - Train Acc: 0.6560 - Val Acc: 0.5094\n",
      "Epoch [70/80] - Loss: 0.9121 - Train Acc: 0.6550 - Val Acc: 0.5113\n",
      "Epoch [71/80] - Loss: 0.8934 - Train Acc: 0.6508 - Val Acc: 0.5057\n",
      "Epoch [72/80] - Loss: 0.8953 - Train Acc: 0.6470 - Val Acc: 0.5038\n",
      "Epoch [73/80] - Loss: 0.8803 - Train Acc: 0.6602 - Val Acc: 0.5057\n",
      "Epoch [74/80] - Loss: 0.8764 - Train Acc: 0.6546 - Val Acc: 0.5019\n",
      "Epoch [75/80] - Loss: 0.8945 - Train Acc: 0.6432 - Val Acc: 0.5094\n",
      "Epoch [76/80] - Loss: 0.9074 - Train Acc: 0.6338 - Val Acc: 0.5000\n",
      "Epoch [77/80] - Loss: 0.8779 - Train Acc: 0.6494 - Val Acc: 0.5019\n",
      "Epoch [78/80] - Loss: 0.8967 - Train Acc: 0.6579 - Val Acc: 0.5132\n",
      "Epoch [79/80] - Loss: 0.8951 - Train Acc: 0.6451 - Val Acc: 0.5057\n",
      "Epoch [80/80] - Loss: 0.8764 - Train Acc: 0.6621 - Val Acc: 0.5094\n",
      "\n",
      "===== Fold 4 =====\n",
      "Epoch [1/80] - Loss: 1.8096 - Train Acc: 0.2742 - Val Acc: 0.2566\n",
      "Epoch [2/80] - Loss: 1.5929 - Train Acc: 0.3365 - Val Acc: 0.3566\n",
      "Epoch [3/80] - Loss: 1.4977 - Train Acc: 0.3719 - Val Acc: 0.3283\n",
      "Epoch [4/80] - Loss: 1.3426 - Train Acc: 0.4644 - Val Acc: 0.4358\n",
      "Epoch [5/80] - Loss: 1.4065 - Train Acc: 0.4294 - Val Acc: 0.3830\n",
      "Epoch [6/80] - Loss: 1.3448 - Train Acc: 0.4545 - Val Acc: 0.3642\n",
      "Epoch [7/80] - Loss: 1.3127 - Train Acc: 0.4748 - Val Acc: 0.4226\n",
      "Epoch [8/80] - Loss: 1.3024 - Train Acc: 0.4927 - Val Acc: 0.4434\n",
      "Epoch [9/80] - Loss: 1.2605 - Train Acc: 0.4979 - Val Acc: 0.3774\n",
      "Epoch [10/80] - Loss: 1.2454 - Train Acc: 0.5059 - Val Acc: 0.4075\n",
      "Epoch [11/80] - Loss: 1.1567 - Train Acc: 0.5531 - Val Acc: 0.4547\n",
      "Epoch [12/80] - Loss: 1.1114 - Train Acc: 0.5555 - Val Acc: 0.4698\n",
      "Epoch [13/80] - Loss: 1.0931 - Train Acc: 0.5739 - Val Acc: 0.4755\n",
      "Epoch [14/80] - Loss: 1.1175 - Train Acc: 0.5672 - Val Acc: 0.5132\n",
      "Epoch [15/80] - Loss: 1.0737 - Train Acc: 0.5861 - Val Acc: 0.5189\n",
      "Epoch [16/80] - Loss: 1.0608 - Train Acc: 0.5885 - Val Acc: 0.5075\n",
      "Epoch [17/80] - Loss: 1.0399 - Train Acc: 0.6022 - Val Acc: 0.5189\n",
      "Epoch [18/80] - Loss: 1.0344 - Train Acc: 0.5979 - Val Acc: 0.4811\n",
      "Epoch [19/80] - Loss: 1.0337 - Train Acc: 0.5941 - Val Acc: 0.5132\n",
      "Epoch [20/80] - Loss: 1.0221 - Train Acc: 0.6045 - Val Acc: 0.5113\n",
      "Epoch [21/80] - Loss: 0.9736 - Train Acc: 0.6121 - Val Acc: 0.5302\n",
      "Epoch [22/80] - Loss: 0.9672 - Train Acc: 0.6262 - Val Acc: 0.4943\n",
      "Epoch [23/80] - Loss: 0.9814 - Train Acc: 0.6126 - Val Acc: 0.5075\n",
      "Epoch [24/80] - Loss: 0.9550 - Train Acc: 0.6291 - Val Acc: 0.5377\n",
      "Epoch [25/80] - Loss: 1.0097 - Train Acc: 0.6055 - Val Acc: 0.5132\n",
      "Epoch [26/80] - Loss: 0.9823 - Train Acc: 0.6220 - Val Acc: 0.4906\n",
      "Epoch [27/80] - Loss: 0.9595 - Train Acc: 0.6210 - Val Acc: 0.5057\n",
      "Epoch [28/80] - Loss: 0.9416 - Train Acc: 0.6286 - Val Acc: 0.5491\n",
      "Epoch [29/80] - Loss: 0.9383 - Train Acc: 0.6324 - Val Acc: 0.5434\n",
      "Epoch [30/80] - Loss: 0.9057 - Train Acc: 0.6494 - Val Acc: 0.5170\n",
      "Epoch [31/80] - Loss: 0.9070 - Train Acc: 0.6470 - Val Acc: 0.5226\n",
      "Epoch [32/80] - Loss: 0.8927 - Train Acc: 0.6470 - Val Acc: 0.5472\n",
      "Epoch [33/80] - Loss: 0.9078 - Train Acc: 0.6626 - Val Acc: 0.5226\n",
      "Epoch [34/80] - Loss: 0.8783 - Train Acc: 0.6531 - Val Acc: 0.5245\n",
      "Epoch [35/80] - Loss: 0.8650 - Train Acc: 0.6564 - Val Acc: 0.5245\n",
      "Epoch [36/80] - Loss: 0.8768 - Train Acc: 0.6630 - Val Acc: 0.5472\n",
      "Epoch [37/80] - Loss: 0.8555 - Train Acc: 0.6616 - Val Acc: 0.5434\n",
      "Epoch [38/80] - Loss: 0.8623 - Train Acc: 0.6531 - Val Acc: 0.5321\n",
      "Epoch [39/80] - Loss: 0.8496 - Train Acc: 0.6649 - Val Acc: 0.5528\n",
      "Epoch [40/80] - Loss: 0.8515 - Train Acc: 0.6673 - Val Acc: 0.5434\n",
      "Epoch [41/80] - Loss: 0.8415 - Train Acc: 0.6692 - Val Acc: 0.5566\n",
      "Epoch [42/80] - Loss: 0.8321 - Train Acc: 0.6715 - Val Acc: 0.5472\n",
      "Epoch [43/80] - Loss: 0.8184 - Train Acc: 0.6815 - Val Acc: 0.5623\n",
      "Epoch [44/80] - Loss: 0.8232 - Train Acc: 0.6748 - Val Acc: 0.5415\n",
      "Epoch [45/80] - Loss: 0.8251 - Train Acc: 0.6744 - Val Acc: 0.5472\n",
      "Epoch [46/80] - Loss: 0.8305 - Train Acc: 0.6706 - Val Acc: 0.5396\n",
      "Epoch [47/80] - Loss: 0.8377 - Train Acc: 0.6800 - Val Acc: 0.5472\n",
      "Epoch [48/80] - Loss: 0.8180 - Train Acc: 0.6739 - Val Acc: 0.5566\n",
      "Epoch [49/80] - Loss: 0.8281 - Train Acc: 0.6805 - Val Acc: 0.5660\n",
      "Epoch [50/80] - Loss: 0.8001 - Train Acc: 0.6885 - Val Acc: 0.5340\n",
      "Epoch [51/80] - Loss: 0.8047 - Train Acc: 0.6819 - Val Acc: 0.5415\n",
      "Epoch [52/80] - Loss: 0.7973 - Train Acc: 0.6909 - Val Acc: 0.5377\n",
      "Epoch [53/80] - Loss: 0.7970 - Train Acc: 0.6838 - Val Acc: 0.5491\n",
      "Epoch [54/80] - Loss: 0.8017 - Train Acc: 0.6876 - Val Acc: 0.5377\n",
      "Epoch [55/80] - Loss: 0.7848 - Train Acc: 0.6928 - Val Acc: 0.5491\n",
      "Epoch [56/80] - Loss: 0.8071 - Train Acc: 0.6815 - Val Acc: 0.5453\n",
      "Epoch [57/80] - Loss: 0.7928 - Train Acc: 0.6899 - Val Acc: 0.5547\n",
      "Epoch [58/80] - Loss: 0.7783 - Train Acc: 0.6984 - Val Acc: 0.5528\n",
      "Epoch [59/80] - Loss: 0.8005 - Train Acc: 0.6791 - Val Acc: 0.5472\n",
      "Epoch [60/80] - Loss: 0.7929 - Train Acc: 0.6871 - Val Acc: 0.5566\n",
      "Epoch [61/80] - Loss: 0.7917 - Train Acc: 0.6862 - Val Acc: 0.5528\n",
      "Epoch [62/80] - Loss: 0.7973 - Train Acc: 0.6970 - Val Acc: 0.5528\n",
      "Epoch [63/80] - Loss: 0.7821 - Train Acc: 0.6881 - Val Acc: 0.5566\n",
      "Epoch [64/80] - Loss: 0.7952 - Train Acc: 0.6923 - Val Acc: 0.5453\n",
      "Epoch [65/80] - Loss: 0.7645 - Train Acc: 0.6966 - Val Acc: 0.5585\n",
      "Epoch [66/80] - Loss: 0.7956 - Train Acc: 0.6994 - Val Acc: 0.5566\n",
      "Epoch [67/80] - Loss: 0.7665 - Train Acc: 0.6928 - Val Acc: 0.5585\n",
      "Epoch [68/80] - Loss: 0.7773 - Train Acc: 0.6928 - Val Acc: 0.5698\n",
      "Epoch [69/80] - Loss: 0.7921 - Train Acc: 0.6852 - Val Acc: 0.5755\n",
      "Epoch [70/80] - Loss: 0.7821 - Train Acc: 0.6914 - Val Acc: 0.5717\n",
      "Epoch [71/80] - Loss: 0.8464 - Train Acc: 0.6928 - Val Acc: 0.5698\n",
      "Epoch [72/80] - Loss: 0.7782 - Train Acc: 0.7069 - Val Acc: 0.5755\n",
      "Epoch [73/80] - Loss: 0.7899 - Train Acc: 0.7050 - Val Acc: 0.5755\n",
      "Epoch [74/80] - Loss: 0.7759 - Train Acc: 0.6975 - Val Acc: 0.5698\n",
      "Epoch [75/80] - Loss: 0.7854 - Train Acc: 0.7079 - Val Acc: 0.5717\n",
      "Epoch [76/80] - Loss: 0.7774 - Train Acc: 0.7022 - Val Acc: 0.5679\n",
      "Epoch [77/80] - Loss: 0.7786 - Train Acc: 0.6914 - Val Acc: 0.5698\n",
      "Epoch [78/80] - Loss: 0.7626 - Train Acc: 0.7192 - Val Acc: 0.5698\n",
      "Epoch [79/80] - Loss: 0.7614 - Train Acc: 0.7027 - Val Acc: 0.5642\n",
      "Epoch [80/80] - Loss: 0.7787 - Train Acc: 0.6980 - Val Acc: 0.5679\n",
      "\n",
      "===== Fold 5 =====\n",
      "Epoch [1/80] - Loss: 1.8001 - Train Acc: 0.2821 - Val Acc: 0.3157\n",
      "Epoch [2/80] - Loss: 1.5410 - Train Acc: 0.3608 - Val Acc: 0.3289\n",
      "Epoch [3/80] - Loss: 1.5516 - Train Acc: 0.3566 - Val Acc: 0.3422\n",
      "Epoch [4/80] - Loss: 1.4982 - Train Acc: 0.3736 - Val Acc: 0.3308\n",
      "Epoch [5/80] - Loss: 1.4186 - Train Acc: 0.4142 - Val Acc: 0.3913\n",
      "Epoch [6/80] - Loss: 1.4293 - Train Acc: 0.4132 - Val Acc: 0.4064\n",
      "Epoch [7/80] - Loss: 1.3704 - Train Acc: 0.4528 - Val Acc: 0.3308\n",
      "Epoch [8/80] - Loss: 1.2979 - Train Acc: 0.4962 - Val Acc: 0.4310\n",
      "Epoch [9/80] - Loss: 1.2166 - Train Acc: 0.5358 - Val Acc: 0.4934\n",
      "Epoch [10/80] - Loss: 1.2524 - Train Acc: 0.5123 - Val Acc: 0.5161\n",
      "Epoch [11/80] - Loss: 1.1417 - Train Acc: 0.5542 - Val Acc: 0.5066\n",
      "Epoch [12/80] - Loss: 1.1560 - Train Acc: 0.5425 - Val Acc: 0.4991\n",
      "Epoch [13/80] - Loss: 1.0807 - Train Acc: 0.5792 - Val Acc: 0.5841\n",
      "Epoch [14/80] - Loss: 1.0944 - Train Acc: 0.5679 - Val Acc: 0.5104\n",
      "Epoch [15/80] - Loss: 1.0795 - Train Acc: 0.5769 - Val Acc: 0.5520\n",
      "Epoch [16/80] - Loss: 1.0609 - Train Acc: 0.5929 - Val Acc: 0.5728\n",
      "Epoch [17/80] - Loss: 1.0556 - Train Acc: 0.5835 - Val Acc: 0.5350\n",
      "Epoch [18/80] - Loss: 1.0757 - Train Acc: 0.5873 - Val Acc: 0.5104\n",
      "Epoch [19/80] - Loss: 1.0048 - Train Acc: 0.6127 - Val Acc: 0.5501\n",
      "Epoch [20/80] - Loss: 1.0157 - Train Acc: 0.6038 - Val Acc: 0.5463\n",
      "Epoch [21/80] - Loss: 0.9420 - Train Acc: 0.6363 - Val Acc: 0.5992\n",
      "Epoch [22/80] - Loss: 0.9356 - Train Acc: 0.6316 - Val Acc: 0.5955\n",
      "Epoch [23/80] - Loss: 0.9283 - Train Acc: 0.6415 - Val Acc: 0.5974\n",
      "Epoch [24/80] - Loss: 0.9254 - Train Acc: 0.6472 - Val Acc: 0.5822\n",
      "Epoch [25/80] - Loss: 0.9039 - Train Acc: 0.6599 - Val Acc: 0.5898\n",
      "Epoch [26/80] - Loss: 0.9156 - Train Acc: 0.6500 - Val Acc: 0.5633\n",
      "Epoch [27/80] - Loss: 0.9142 - Train Acc: 0.6410 - Val Acc: 0.5992\n",
      "Epoch [28/80] - Loss: 0.9018 - Train Acc: 0.6434 - Val Acc: 0.5822\n",
      "Epoch [29/80] - Loss: 0.8968 - Train Acc: 0.6524 - Val Acc: 0.5520\n",
      "Epoch [30/80] - Loss: 0.9387 - Train Acc: 0.6387 - Val Acc: 0.5690\n",
      "Epoch [31/80] - Loss: 0.8905 - Train Acc: 0.6623 - Val Acc: 0.5728\n",
      "Epoch [32/80] - Loss: 0.8823 - Train Acc: 0.6552 - Val Acc: 0.5501\n",
      "Epoch [33/80] - Loss: 0.8900 - Train Acc: 0.6519 - Val Acc: 0.5747\n",
      "Epoch [34/80] - Loss: 0.8676 - Train Acc: 0.6670 - Val Acc: 0.5558\n",
      "Epoch [35/80] - Loss: 0.8424 - Train Acc: 0.6778 - Val Acc: 0.5766\n",
      "Epoch [36/80] - Loss: 0.8583 - Train Acc: 0.6651 - Val Acc: 0.5766\n",
      "Epoch [37/80] - Loss: 0.8452 - Train Acc: 0.6722 - Val Acc: 0.5936\n",
      "Epoch [38/80] - Loss: 0.8548 - Train Acc: 0.6665 - Val Acc: 0.6068\n",
      "Epoch [39/80] - Loss: 0.8296 - Train Acc: 0.6708 - Val Acc: 0.5860\n",
      "Epoch [40/80] - Loss: 0.8405 - Train Acc: 0.6656 - Val Acc: 0.5974\n",
      "Epoch [41/80] - Loss: 0.8084 - Train Acc: 0.6844 - Val Acc: 0.5860\n",
      "Epoch [42/80] - Loss: 0.7861 - Train Acc: 0.6939 - Val Acc: 0.5803\n",
      "Epoch [43/80] - Loss: 0.7839 - Train Acc: 0.6825 - Val Acc: 0.5917\n",
      "Epoch [44/80] - Loss: 0.7854 - Train Acc: 0.6991 - Val Acc: 0.5898\n",
      "Epoch [45/80] - Loss: 0.7740 - Train Acc: 0.7085 - Val Acc: 0.5917\n",
      "Epoch [46/80] - Loss: 0.7784 - Train Acc: 0.7005 - Val Acc: 0.5974\n",
      "Epoch [47/80] - Loss: 0.7922 - Train Acc: 0.6835 - Val Acc: 0.5898\n",
      "Epoch [48/80] - Loss: 0.7887 - Train Acc: 0.6981 - Val Acc: 0.5784\n",
      "Epoch [49/80] - Loss: 0.7790 - Train Acc: 0.6981 - Val Acc: 0.6011\n",
      "Epoch [50/80] - Loss: 0.7660 - Train Acc: 0.7047 - Val Acc: 0.5917\n",
      "Epoch [51/80] - Loss: 0.7559 - Train Acc: 0.7104 - Val Acc: 0.6030\n",
      "Epoch [52/80] - Loss: 0.7634 - Train Acc: 0.7047 - Val Acc: 0.5992\n",
      "Epoch [53/80] - Loss: 0.7548 - Train Acc: 0.7005 - Val Acc: 0.6011\n",
      "Epoch [54/80] - Loss: 0.7591 - Train Acc: 0.6967 - Val Acc: 0.5974\n",
      "Epoch [55/80] - Loss: 0.7463 - Train Acc: 0.7127 - Val Acc: 0.6011\n",
      "Epoch [56/80] - Loss: 0.7587 - Train Acc: 0.7080 - Val Acc: 0.5992\n",
      "Epoch [57/80] - Loss: 0.7447 - Train Acc: 0.7146 - Val Acc: 0.5974\n",
      "Epoch [58/80] - Loss: 0.7518 - Train Acc: 0.7099 - Val Acc: 0.5974\n",
      "Epoch [59/80] - Loss: 0.7385 - Train Acc: 0.7212 - Val Acc: 0.6011\n",
      "Epoch [60/80] - Loss: 0.7412 - Train Acc: 0.7090 - Val Acc: 0.6087\n",
      "Epoch [61/80] - Loss: 0.7361 - Train Acc: 0.7028 - Val Acc: 0.6106\n",
      "Epoch [62/80] - Loss: 0.7398 - Train Acc: 0.7137 - Val Acc: 0.6030\n",
      "Epoch [63/80] - Loss: 0.7464 - Train Acc: 0.7198 - Val Acc: 0.6068\n",
      "Epoch [64/80] - Loss: 0.7340 - Train Acc: 0.7156 - Val Acc: 0.6049\n",
      "Epoch [65/80] - Loss: 0.7343 - Train Acc: 0.7203 - Val Acc: 0.5936\n",
      "Epoch [66/80] - Loss: 0.7391 - Train Acc: 0.7146 - Val Acc: 0.5936\n",
      "Epoch [67/80] - Loss: 0.7396 - Train Acc: 0.7071 - Val Acc: 0.5974\n",
      "Epoch [68/80] - Loss: 0.7489 - Train Acc: 0.7170 - Val Acc: 0.5974\n",
      "Epoch [69/80] - Loss: 0.7366 - Train Acc: 0.7231 - Val Acc: 0.6049\n",
      "Epoch [70/80] - Loss: 0.7375 - Train Acc: 0.7094 - Val Acc: 0.5992\n",
      "Epoch [71/80] - Loss: 0.7296 - Train Acc: 0.7132 - Val Acc: 0.6087\n",
      "Epoch [72/80] - Loss: 0.7346 - Train Acc: 0.7123 - Val Acc: 0.5992\n",
      "Epoch [73/80] - Loss: 0.7147 - Train Acc: 0.7146 - Val Acc: 0.5992\n",
      "Epoch [74/80] - Loss: 0.7262 - Train Acc: 0.7193 - Val Acc: 0.5974\n",
      "Epoch [75/80] - Loss: 0.7281 - Train Acc: 0.7245 - Val Acc: 0.6011\n",
      "Epoch [76/80] - Loss: 0.7172 - Train Acc: 0.7231 - Val Acc: 0.5992\n",
      "Epoch [77/80] - Loss: 0.7192 - Train Acc: 0.7241 - Val Acc: 0.5992\n",
      "Epoch [78/80] - Loss: 0.7174 - Train Acc: 0.7231 - Val Acc: 0.6087\n",
      "Epoch [79/80] - Loss: 0.7264 - Train Acc: 0.7146 - Val Acc: 0.6011\n",
      "Epoch [80/80] - Loss: 0.7146 - Train Acc: 0.7217 - Val Acc: 0.6049\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load preprocessed data\n",
    "X = np.load(\"X_sequences.npy\", allow_pickle=True)\n",
    "y = np.load(\"y_labels.npy\", allow_pickle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(type(X))          # Likely <class 'list'>\n",
    "print(type(X[0]))\n",
    "print(device)        # Works only if X[0] is ndarray\n",
    "\n",
    "X = [np.array(seq) for seq in X]\n",
    "num_features = X[0].shape[1]\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "    X = np.array([np.array(seq) for seq in X], dtype=object)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Now this works:\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # X_train, X_val = X[train_idx], X[val_idx]\n",
    "    # y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    train_dataset = PoseSequenceDataset(X_train, y_train)\n",
    "    val_dataset = PoseSequenceDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model1 = PoseSequenceTransformer(num_features=num_features, num_classes=num_classes).to(device)\n",
    "    optimizer = AdamW(model1.parameters(), lr=0.001)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(80):  # could be 100\n",
    "        model1.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_targets = [], []\n",
    "\n",
    "        for seqs, labels in train_loader:\n",
    "            seqs, labels = seqs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model1(seqs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            train_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "\n",
    "        # Validation\n",
    "        model1.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for seqs, labels in val_loader:\n",
    "                seqs, labels = seqs.to(device), labels.to(device)\n",
    "                outputs = model1(seqs)\n",
    "                val_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "                val_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_targets, val_preds)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/80] - Loss: {train_loss/len(train_loader):.4f} - Train Acc: {train_acc:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    fold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(), \"pose_transformer1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b68d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6172\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Fall Down       0.71      0.54      0.62       169\n",
      "  Lying Down       0.64      0.87      0.74       142\n",
      "    Sit down       0.58      0.36      0.45        77\n",
      "     Sitting       0.43      0.61      0.51       120\n",
      "    Stand up       0.53      0.45      0.48       119\n",
      "    Standing       0.60      0.69      0.64       229\n",
      "     Walking       0.74      0.62      0.68       283\n",
      "\n",
      "    accuracy                           0.62      1139\n",
      "   macro avg       0.61      0.59      0.59      1139\n",
      "weighted avg       0.63      0.62      0.61      1139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "X_test = np.load(\"X_test.npy\", allow_pickle=True)\n",
    "y_test = np.load(\"y_test.npy\", allow_pickle=True)\n",
    "\n",
    "X_test = [np.array(seq) for seq in X_test]\n",
    "num_features = X_test[0].shape[1]  \n",
    "num_classes = len(np.unique(y_test))\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "\n",
    "test_dataset = PoseSequenceDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model1 = PoseSequenceTransformer(num_features=num_features, num_classes=num_classes).to(device)\n",
    "\n",
    "model1.load_state_dict(torch.load(\"pose_transformer1.pth\", map_location=device))\n",
    "model1.eval()\n",
    "\n",
    "all_preds, all_targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seqs, labels in test_loader:\n",
    "        seqs, labels = seqs.to(device), labels.to(device)\n",
    "        outputs = model1(seqs)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_targets, all_preds)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6618083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_20092\\3977832452.py:50: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pred_x, pred_y = int(kf.x[0]), int(kf.x[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Action: Sitting\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ===== Parameters =====\n",
    "VIDEO_PATH = r\"D:\\fitz\\fitzpatrick17k-main\\pose\\UCF50\\GolfSwing\\v_GolfSwing_g01_c01.avi\"\n",
    "  # your custom video\n",
    "MODEL_PATH = \"pose_transformer1.pth\"\n",
    "LABELS_PATH = \"y_labels.npy\"    # from training\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===== Load label encoder from training labels =====\n",
    "y_train = np.load(LABELS_PATH, allow_pickle=True)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "# ===== Mediapipe setup =====\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# ===== Feature extraction for a single video =====\n",
    "def extract_features_from_video(video_path):\n",
    "    pose_kalman_filters = {}\n",
    "    joint_history = {}\n",
    "    angle_history = {}\n",
    "    status_history = {}\n",
    "\n",
    "    frames = preprocess_video(video_path, background_subtraction=False)\n",
    "    sequence_features = []\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        for frame in frames:\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(image)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                frame_features = []\n",
    "                for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "                    x, y = int(lm.x * frame.shape[1]), int(lm.y * frame.shape[0])\n",
    "\n",
    "                    key = f'joint_{idx}'\n",
    "                    if key not in pose_kalman_filters:\n",
    "                        pose_kalman_filters[key] = create_kalman_filter()\n",
    "                        pose_kalman_filters[key].x[:2] = np.array([[x], [y]])\n",
    "                    kf = pose_kalman_filters[key]\n",
    "                    kf.predict()\n",
    "                    kf.update(np.array([x, y]))\n",
    "                    pred_x, pred_y = int(kf.x[0]), int(kf.x[1])\n",
    "\n",
    "                    frame_features.extend([pred_x, pred_y])\n",
    "\n",
    "                # Angles + status\n",
    "                for name, (a_idx, b_idx, c_idx) in ANGLE_JOINTS.items():\n",
    "                    if all(joint_history.get(f'joint_{i}', []) for i in (a_idx, b_idx, c_idx)):\n",
    "                        a = (pose_kalman_filters[f'joint_{a_idx}'].x[0], pose_kalman_filters[f'joint_{a_idx}'].x[1])\n",
    "                        b = (pose_kalman_filters[f'joint_{b_idx}'].x[0], pose_kalman_filters[f'joint_{b_idx}'].x[1])\n",
    "                        c = (pose_kalman_filters[f'joint_{c_idx}'].x[0], pose_kalman_filters[f'joint_{c_idx}'].x[1])\n",
    "\n",
    "                        angle = calculate_angle(a, b, c)\n",
    "                        delta_y = b[1] - joint_history.get(f'joint_{b_idx}', [(0,0)])[-1][1]\n",
    "                        angle_velocity = 0\n",
    "                        if name in angle_history and len(angle_history[name]) > 0:\n",
    "                            angle_velocity = angle - angle_history[name][-1]\n",
    "\n",
    "                        label = classify_joint_movement(delta_y, angle_velocity)\n",
    "                        frame_features.extend([angle, {\"UP\":1, \"DOWN\":-1, \"STATIC\":0}[label]])\n",
    "                    else:\n",
    "                        frame_features.extend([0, 0])\n",
    "\n",
    "                sequence_features.append(frame_features)\n",
    "\n",
    "    return np.array(sequence_features, dtype=np.float32)  # [T, 78]\n",
    "\n",
    "# ===== Extract features for this video =====\n",
    "features = extract_features_from_video(VIDEO_PATH)\n",
    "\n",
    "# ===== Load model =====\n",
    "num_features = features.shape[1]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = PoseSequenceTransformer(num_features=num_features, num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ===== Prepare for prediction =====\n",
    "seq_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(device)  # [1, T, 78]\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(seq_tensor)\n",
    "    pred_class = torch.argmax(output, dim=1).cpu().item()\n",
    "    pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "\n",
    "print(f\"Predicted Action: {pred_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e2073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "VIDEO_PATH = r\"D:\\fitz\\fitzpatrick17k-main\\pose\\UCF50\\video_1.avi\"\n",
    "MODEL_PATH = \"pose_transformer1.pth\"\n",
    "LABELS_PATH = \"y_labels.npy\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "\n",
    "y_train = np.load(LABELS_PATH, allow_pickle=True)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "\n",
    "num_features = 78  \n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = PoseSequenceTransformer(num_features=num_features, num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "pose_kalman_filters = {}\n",
    "joint_history = {}\n",
    "angle_history = {}\n",
    "status_history = {}\n",
    "\n",
    "frames = preprocess_video(VIDEO_PATH, background_subtraction=False)\n",
    "video_writer = cv2.VideoWriter(\"prediction_output.avi\", cv2.VideoWriter_fourcc(*'XVID'), 20,\n",
    "                               (frames[0].shape[1], frames[0].shape[0]))\n",
    "\n",
    "sequence_features = []\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    for frame in frames:\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "        )\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            frame_features = []\n",
    "            for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "                x, y = int(lm.x * image.shape[1]), int(lm.y * image.shape[0])\n",
    "                key = f'joint_{idx}'\n",
    "\n",
    "                if key not in pose_kalman_filters:\n",
    "                    pose_kalman_filters[key] = create_kalman_filter()\n",
    "                    pose_kalman_filters[key].x[:2] = np.array([[x], [y]])\n",
    "                kf = pose_kalman_filters[key]\n",
    "                kf.predict()\n",
    "                kf.update(np.array([x, y]))\n",
    "                pred_x, pred_y = int(kf.x[0]), int(kf.x[1])\n",
    "\n",
    "                if key not in joint_history:\n",
    "                    joint_history[key] = []\n",
    "                joint_history[key].append((pred_x, pred_y))\n",
    "\n",
    "                frame_features.extend([pred_x, pred_y])\n",
    "\n",
    "            # Angles + status\n",
    "            for name, (a_idx, b_idx, c_idx) in ANGLE_JOINTS.items():\n",
    "                if all(len(joint_history.get(f'joint_{i}', [])) >= 2 for i in (a_idx, b_idx, c_idx)):\n",
    "                    a = joint_history[f'joint_{a_idx}'][-1]\n",
    "                    b = joint_history[f'joint_{b_idx}'][-1]\n",
    "                    c = joint_history[f'joint_{c_idx}'][-1]\n",
    "\n",
    "                    angle = calculate_angle(a, b, c)\n",
    "                    angle_history.setdefault(name, []).append(angle)\n",
    "\n",
    "                    delta_y = b[1] - joint_history[f'joint_{b_idx}'][-2][1]\n",
    "                    angle_velocity = angle - angle_history[name][-2] if len(angle_history[name]) > 1 else 0\n",
    "                    label = classify_joint_movement(delta_y, angle_velocity)\n",
    "\n",
    "                    status_history.setdefault(name, []).append(label)\n",
    "\n",
    "                    frame_features.extend([angle, {\"UP\":1, \"DOWN\":-1, \"STATIC\":0, \"TRANSITION\":1}[label]])\n",
    "                else:\n",
    "                    frame_features.extend([0, 0])\n",
    "\n",
    "            sequence_features.append(frame_features)\n",
    "\n",
    "        # ==== Prediction every 10 frames ====\n",
    "        if len(sequence_features) >= 10:\n",
    "            feat_array = np.array(sequence_features, dtype=np.float32)\n",
    "            seq_tensor = torch.tensor(feat_array, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(seq_tensor)\n",
    "                pred_class = torch.argmax(output, dim=1).cpu().item()\n",
    "                pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "            cv2.putText(image, f\"Action: {pred_label}\",\n",
    "                        (30, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "\n",
    "        video_writer.write(image)\n",
    "        cv2.imshow('Pose Tracking + Prediction', image)\n",
    "\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video_writer.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6de5b5",
   "metadata": {},
   "source": [
    "# Final code for activity tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a48500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import csv\n",
    "\n",
    "VIDEO_PATH = r\"D:\\fitz\\fitzpatrick17k-main\\pose\\UCF50\\video_1.avi\"\n",
    "MODEL_PATH = \"pose_transformer1.pth\"\n",
    "LABELS_PATH = \"y_labels.npy\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "\n",
    "def draw_trajectories(image, joint_history, max_len=20):\n",
    "    \"\"\"Draw trailing lines showing movement history for each joint.\"\"\"\n",
    "    for key, points in joint_history.items():\n",
    "        if len(points) < 2:\n",
    "            continue\n",
    "        color = (0, 255, 255)  # Cyan color for trails\n",
    "        for i in range(max(0, len(points) - max_len), len(points) - 1):\n",
    "            cv2.line(image, points[i], points[i + 1], color, 2)\n",
    "\n",
    "\n",
    "def save_csv_log(filename, angle_history, joint_history, status_history):\n",
    "    \"\"\"Save angle, joint positions, and status to CSV file.\"\"\"\n",
    "    with open(filename, mode='w', newline='') as csv_file:\n",
    "        fieldnames = ['frame', 'joint', 'angle', 'x', 'y', 'status']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        frames = len(next(iter(angle_history.values()), []))\n",
    "        for frame_idx in range(frames):\n",
    "            for joint_name in ANGLE_JOINTS.keys():\n",
    "                angle = angle_history.get(joint_name, [None]*frames)[frame_idx]\n",
    "                # Use last known joint b (middle joint) position as example\n",
    "                b_idx = ANGLE_JOINTS[joint_name][1]\n",
    "                pos = joint_history.get(f'joint_{b_idx}', [(None, None)]*frames)[frame_idx]\n",
    "                status = status_history.get(joint_name, [\"\"]*frames)[frame_idx]\n",
    "                writer.writerow({\n",
    "                    'frame': frame_idx,\n",
    "                    'joint': joint_name,\n",
    "                    'angle': angle,\n",
    "                    'x': pos[0],\n",
    "                    'y': pos[1],\n",
    "                    'status': status\n",
    "                })\n",
    "\n",
    "# ===== Load label encoder =====\n",
    "y_train = np.load(LABELS_PATH, allow_pickle=True)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "# ===== Load trained model =====\n",
    "num_features = 78  # Must match training\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = PoseSequenceTransformer(num_features=num_features, num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ===== Kalman filter + histories =====\n",
    "pose_kalman_filters = {}\n",
    "joint_history = {}\n",
    "angle_history = {}\n",
    "status_history = {}\n",
    "\n",
    "# ===== Video setup =====\n",
    "frames = preprocess_video(VIDEO_PATH, background_subtraction=False)\n",
    "frame_width, frame_height = frames[0].shape[1], frames[0].shape[0]\n",
    "fps = 20\n",
    "video_writer = cv2.VideoWriter(\"prediction_output.avi\", cv2.VideoWriter_fourcc(*'XVID'), fps,\n",
    "                               (frame_width, frame_height))\n",
    "\n",
    "sequence_features = []\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    for frame_idx, frame in enumerate(frames):\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw pose landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "        )\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            frame_features = []\n",
    "            for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "                x, y = int(lm.x * image.shape[1]), int(lm.y * image.shape[0])\n",
    "                key = f'joint_{idx}'\n",
    "\n",
    "                if key not in pose_kalman_filters:\n",
    "                    pose_kalman_filters[key] = create_kalman_filter()\n",
    "                    pose_kalman_filters[key].x[:2] = np.array([[x], [y]])\n",
    "                kf = pose_kalman_filters[key]\n",
    "                kf.predict()\n",
    "                kf.update(np.array([x, y]))\n",
    "                pred_x, pred_y = int(kf.x[0]), int(kf.x[1])\n",
    "\n",
    "                if key not in joint_history:\n",
    "                    joint_history[key] = []\n",
    "                joint_history[key].append((pred_x, pred_y))\n",
    "\n",
    "                frame_features.extend([pred_x, pred_y])\n",
    "\n",
    "            # Angles + status + visualization\n",
    "            for name, (a_idx, b_idx, c_idx) in ANGLE_JOINTS.items():\n",
    "                if all(len(joint_history.get(f'joint_{i}', [])) >= 2 for i in (a_idx, b_idx, c_idx)):\n",
    "                    a = joint_history[f'joint_{a_idx}'][-1]\n",
    "                    b = joint_history[f'joint_{b_idx}'][-1]\n",
    "                    c = joint_history[f'joint_{c_idx}'][-1]\n",
    "\n",
    "                    angle = calculate_angle(a, b, c)\n",
    "                    angle_history.setdefault(name, []).append(angle)\n",
    "\n",
    "                    delta_y = b[1] - joint_history[f'joint_{b_idx}'][-2][1]\n",
    "                    angle_velocity = angle - angle_history[name][-2] if len(angle_history[name]) > 1 else 0\n",
    "                    label = classify_joint_movement(delta_y, angle_velocity) or \"STATIC\"\n",
    "\n",
    "                    status_history.setdefault(name, []).append(label)\n",
    "\n",
    "                    movement_value = {\"UP\": 1, \"DOWN\": -1, \"STATIC\": 0, \"TRANSITION\": 1}.get(label, 0)\n",
    "                    frame_features.extend([angle, movement_value])\n",
    "\n",
    "                    # Show joint angle + label on frame\n",
    "                    cv2.putText(image, f\"{name}: {label} {int(angle)}°\",\n",
    "                                (b[0] + 10, b[1] - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                (0, 255, 255), 1)\n",
    "                else:\n",
    "                    frame_features.extend([0, 0])\n",
    "\n",
    "            sequence_features.append(frame_features)\n",
    "\n",
    "        # ==== Prediction every 10 frames ====\n",
    "        if len(sequence_features) >= 10:\n",
    "            feat_array = np.array(sequence_features, dtype=np.float32)\n",
    "            seq_tensor = torch.tensor(feat_array, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(seq_tensor)\n",
    "                pred_class = torch.argmax(output, dim=1).cpu().item()\n",
    "                pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "            cv2.putText(image, f\"Action: {pred_label}\",\n",
    "                        (30, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw movement trajectories\n",
    "        # draw_trajectories(image, joint_history)\n",
    "\n",
    "        video_writer.write(image)\n",
    "        cv2.imshow('Pose Tracking + Prediction', image)\n",
    "\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "save_csv_log(\"joint_angle_status_log.csv\", angle_history, joint_history, status_history)\n",
    "\n",
    "video_writer.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fzenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
